---
title: "Introduction to Machine Learning"
output: 
    pdf_document:
        toc: true
    fontsize: 11pt
    geometry: margin=1in
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
# make this an external chunk that can be included in any file
require(knitr)
require(xtable)
require(ISLR)
options(xtable.comment = TRUE)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

#Add Table Captions
#table <- [OUTPUT]
#```{r echo = FALSE, results='asis'}
#print(xtable(table, caption = "caption"), comment = FALSE)
#```

```

\newpage

# Chapter 1: What is Machine Learning   
## Introduction  

__Machine Learnng:__ 

* Explores the construction and usage of algorithms. 
* Improves performance as it receives __more__ infomrtion. 
* Experience comes from observations on how particular problems have been previously solved.

No matter what algorithm used, the primary concept for Machine Learning is __imput knowledge__ or __Data__. Typically this data is a __dataset__ containing a number of observations, each having a number of well defined variables (often called features) :

![alt text](figures/data.png)

From the figure above, we see that each square (row) and its corresponding color is an __observations__. The __features__ in this case are the `size` and `edge` and the `color` is the __label__. In a __R__, the `data.frame()` function is used to depict the dataset above. 

```{r data, echo = TRUE, results = 'asis'}
squares <- data.frame(size = c("small", "big", "medium"),
                      edge = c("dotted", "stripped", "normal"),
                      color = c("green", "yellow", "green"))
print(xtable(squares, caption = "The dataset as a data.rame"), comment = FALSE)

```

The __observations__ correspond to the rows and the columns correspond to the __variables__. 

So the goal of Machine Learning, based on data shown in the example, is to build a __Model of Prediction__. Build a model that can help make predictions about the data for future instances of similar problems. But before the model can be built, one firstly has to acquaint themselves with the Data. The following Excercises deomstrate this process.

### Exercise 1: Getting acquanted with data 

As a first step, we will find out some properties of the dataset with which we will be working. More specifically, we want to know mre about the dataset's number of observations and variables. To do this, we will explore the `iris` dataset. 

#### Instructions:

* Use the two ways presented in the video to find out the number of observations and variables of the iris data set: `str()` and `dim()`. 
* Call `head()` and `tail()` on `iris` to reveal the first and last observations in the iris dataset. 
* Finally, call the `summary()` function to generate a summary of the dataset. What does the printout reveal?

#### Results:

```{r exercise1_1, echo= TRUE}
# The iris is available from the datasets package and is loaded by default
# Reveal number of observations and variables by looking at the structure
str(iris)

# Reveal number of observations and variables by looking at the dimensions
dim(iris)

# Show first and last observations in the iris data set
head(iris)
tail(iris)

# Summarize the iris data set
summary(iris)

```

### Exercise 2: Basic Prediction Model  

To examine a first take a using Machine Learning to make a prediction, we will be using the `Wage` dataset. This dataset contains the wage and some general informaiton for workers in the mid-Atlanic regions of the United States and there could be some relationship between the `age` of a worker and his/her `wage`. Older workers tend to earn more on average than their younger counterparts, hence one could expect an increasing trend in wage as workers age. So we build a linear regression model for you, using the `lm()` function to model the wage of a worker based on his/her age.

With a linear model `lm_wage`, that is built with previous observations, one can predict the wage of new observations. For example, suppose we want to predict the wage of a 60 year old worker. We can use the `predict()` function for this. This generic function takes a model as the first argument. The second argument should be some unseen observations as a data frame. The `predict()` function is then able to predict outcomes for these observations.

#### Instructions:

* Build a Linear Model called `lm_wage`, that models the `wage` by the `age` variable. 
* Create a single colum data frame called `unseen`, with a single column called `age`, containing a single value of `60`. 
* Predict the average wage at age 60 using the `predict()` function.

#### Results:

```{r exercise1_2, echo = TRUE}
# The Wage dataset is already loaded from the outset.
# Build a Linear Model called lm_wage
lm_wage <- lm(wage ~ age, data = Wage)

# Create a data frame for an unseen age (60)
unseen <- data.frame(age = 60)

# Predict the wage of a 60 year old worker
result <- predict(lm_wage, unseen)

```

The Average wage of a 60 year old worker is __`r round(result, digits = 2)` USD__ per day. 

## Classification, Regression and Clustering  

In the majority of Machine Learning problems involve __Classification__, __Regression__ and __Clustering__. 

* A classification problem involves predicting whether a given observation belongs to a certain catagory. What's important to remember regaridng Classification problems, is that the output is __Qualitative__ and the possible classes to which a new observation can belong, are known beforehand (Predefined Classes). 
* A Regression problem involves predicting a continuous, quantitative value based on previous information. The input value are refered to as __Predictors__ and the outout is the __Response__. Regression is somehat simmilar to Classification in that it tries to estimate a funciton that maps the input to the output based on earlier observations, except that the estimate is an actual value. See [Appendix A: Regression Coefficients][]. What's important to remember is that the reponse is ALWAYS __Quantitative__ and the model can only be built with input knowledge of previous input/output observations. 
*  A Clustering problem involves tring to group `similar` observations into clusters, while making sure the clusters thmselves are `dissimilar`. Clustering is similar to Classification, but without saying to wich class the observations have  to belong, therfore no previous knowledge regaring the labels is required.

### Exercise 3: Classification for Filtering Spam

Filtering spam from relevant emails is a typical machine learning task. Information such as word frequency, character frequency and the amount of capital letters can indicate whether an email is spam or not. In the following exercise we work with the dataset `emails` from the [UCI Machine Learning Repository](http://s3.amazonaws.com/assets.datacamp.com/course/intro_to_ml/emails_small.csv). Here, several emails have been labeled by humans as spam (1) or not spam (0) and the results are found in the column `spam`. The considered feature in `emails` is `avg_capital_seq`. It is the average amount of sequential capital letters found in each email. Crete a very basic spamfilter, `spam_classifier()` that uses `avg_capital_seq` to predict whether an email is spam or not. Inspect the `emails` dataset, apply `spam_classifier()` to it and compare the outcome with the true labels. 

#### Instructions:

* Check the dimensions of this dataset, use `dim()`.
* Create the `spam_classifier()` function as a simple set of statements that decide between spam and no spam based on a single input vector.
* Pass the `avg_capital_seq` column of `emails` to `spam_classifier()` and thus determine which emails are spam and which aren't, assigning the resulting outcomes to `spam_pred`.
* Compare the prediction, `spam_pred`, to the true spam labels in `emails` and print out the result. How many of the emails were correctly classified?

#### Results:

```{r exercise1_3, echo = TRUE}
# Load the emails dataset into the workspace
emails <- read.csv("data/emails_small.csv")

# Show the dimensions of emails
dim(emails)

# Inspect definition of spam_classifier()
spam_classifier <- function(x){
  prediction <- rep(NA,length(x))
  prediction[x > 4] <- 1
  prediction[x >= 3 & x <= 4] <- 0
  prediction[x >= 2.2 & x < 3] <- 1
  prediction[x >= 1.4 & x < 2.2] <- 0
  prediction[x > 1.25 & x < 1.4] <- 1
  prediction[x <= 1.25] <- 0
  return(prediction)
}

# Apply the classifier to the avg_capital_seq column as spam_pred
spam_pred <- spam_classifier(emails$avg_capital_seq)

# Compare spam_pred to emails$spam
spam_pred == emails$spam

```

### Exercise 4: LinkedIn Views for the next 3 days using Regression

Analyze the number of views of a LinkedIn profile by predicting how often a profile will be visited in the next 3 days, based on the views for the past 3 weeks. 

#### Instructions:

* Create a sample vector called `linkedin` with a random amount of profile visits.
* Create a vector `days` with the numbers from 1 to 21, which represent the previous 21 days of your linkedin views. 
* Fit a linear model that explains the `linkedin` views based on days. Use the `lm()` function with the appropriate formula. Assign the resulting linear model to `linkedin_lm`. 
* Using this linear model, predict the number of views for the next three days (22, 23 and 24). Use the `predict()` function and the predefined `future_days` data frame. Assign the result to `linkedin_pred`. 
* See how the remaining code plots both the historical data and the predictions. Try to interpret the result.

#### Results:

```{r exercise1_4, echo = TRUE}
# Create the linkedin vector of the number of views per day for 3 weeks
linkedin <- c(5, 7, 4, 9, 11, 10, 14, 17, 13, 11, 18, 17, 21, 21, 24, 23, 28, 35, 21, 27, 23)

# Create the days vector
days <- seq(length(linkedin))

# Fit a linear model called on the linkedin views per day as linkedin_lm
linkedin_lm <- lm(linkedin ~ days)

# Predict the number of views for the next three days: linkedin_pred
future_days <- data.frame(days = 22:24)
linkedin_pred <- predict(linkedin_lm, future_days)

# Plot historical data and predictions
plot(linkedin ~ days, xlim = c(1, 24))
points(22:24, linkedin_pred, col = "green")

```

### Exercise 5: Separateing the Species of Iris using Clustering

This technique tries to group objects. It does this without any prior knowledge of what these groups could or should look like. In this case, the concepts of prior knowledge and unseen observations are less meaningful than for classification and regression. In this exercise, we group irises into 3 distinct clusters, based on several flower characteristics from the `iris` dataset. The clustering itself will be done with the `kmeans()` function.

__Note:__ In problems that have a random aspect (like `kmeans()`), the `set.seed()` function will be used to enforce reproducibility. If you fix the seed, the random numbers that are generated afterwards are always the same.

#### Instructions:

* Create a `data.frame` of Iris characteristics and vector of Iris species. 
* Use the `kmeans()` function. The first argument is `my_iris`; the second argument is `3`, as we want to find three groups in `my_iris`. Assign the result to a new variable `kmeans_iris`. 
* The actual species of the observations is stored in `species`. Use the `table()` function to compare it to the groups the clustering came up with. These groups can be found in the cluster attribute of `kmeans_iris`. 
* Generate a plot of `Petal.Length` against `Petal.Width` and `colors` by cluster.

#### Results:

```{r exercise1_5, echo = TRUE}
# Set random seed.
set.seed(1)

# Create a data.frame of characteristics and vector of species
my_iris <- iris[-5]
species <- iris$Species

# Perform k-means clustering on my_iris as kmeans_iris
kmeans_iris <- kmeans(my_iris, 3)

# Compare the actual Species to the clustering using table()
table <- table(kmeans_iris$cluster)

# Plot Petal.Width against Petal.Length, coloring by cluster
plot(Petal.Length ~ Petal.Width, data = my_iris, col = kmeans_iris$cluster)

```

```{r echo = FALSE, results='asis'}
print(xtable(table, caption = "The clusters that kmeans() came up with"), comment = FALSE)

```

## Supervised vs. Unsupervised Machine Learning 

From the previous techniques, Classification and Regression, we note that are similarities. For both techniques, we try to find a function (or Model) that can later be used to predict labels (or values) for __unseen observations__. It is important that during the training of the function, labeled observations are available to the algorithm and are therefore examples of __Supervised Learning__ techniques. 

Since "labelling" can be tedeous work and is often done by humans, there are other techniques wich don't require labeled observations and are therefore called __Unsupervised Learning__ techniques. Clustering is an example of unsupervised learning in that it finds groups of observations that are similar without requiring a specific label for these observations. 

## Performance of the Model

In Supervised LEarning, the performance of a model can be determined by compaing the __real__ labels of observations with the __predicted__ labels. The goal is to have the models' predictions as close (or simialr) as possible to that of the real data. 

Aternatively, measuring the performance of Unsupervised Learning models is far more difficult, as we don;t have any real labels (or real observations) to compare with.

## Semi-supervised Learning

In Machine LEarning, some techniques overlap between Supervised and Unsupervised learning. For example, our data may contain obersations that are `not` labeled and some that are. To address this kind of dataset:

1. Group similar observations together using __clustering__.
2. Use information about the cluster as well as the other labeled observations to assign a `class` to the unlabelled observations. 
3. This will provide more labeled observations to apply Supervised learning techniques on. 

### Exercise 6: Practical Supervised Learning

In the previous exercises, we used the `kmeans()` function to perform clustering on the `iris` dataset. Additionally, we created our own copy of the dataset by removing the `Species` attribute. Thus we removed the labels of the observations. In this exercise, we will use the same dataset, but instead of dropping the `Species` labels, we will use them do some supervised learning. 

\newpage

# Appendix A: Regression Coefficients

As an example of trying to fit a linear function between a `Predictor` (e.g. Weight) and a `Response` (e.g. Height). $$Height \approx \beta_0+\beta_1\times Weight$$ Together $\beta_1$ and $\beta_0$ are known as the model coefficients or parameters. As soon as we know what these coefficients are, the function is able to convert any new input to output. This means that solving the Machine Learning problem is to find good values for $\beta_1$ and $\beta_0$, estimated from previous input to previous output observations. 

