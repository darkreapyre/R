---
title: "Introduction to Machine Learning"
output: 
    pdf_document:
        toc: true
    fontsize: 11pt
    geometry: margin=1in
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
require(knitr)
require(xtable)
require(ISLR)
require(rpart)
options(xtable.comment = TRUE)
```

\newpage
 
# Chapter 1: What is Machine Learning   
## Introduction  

__Machine Learnng:__ 

* Explores the construction and usage of algorithms. 
* Improves performance as it receives __more__ infomrtion. 
* Experience comes from observations on how particular problems have been previously solved.

No matter what algorithm used, the primary concept for Machine Learning is __imput knowledge__ or __Data__. Typically this data is a __dataset__ containing a number of observations, each having a number of well defined variables (often called features) :

\begin{figure}[h]
\center
\includegraphics{figures/data.png}
\caption{Example of some Data}
\label{Figure1}
\end{figure}

From __Figure 1__, we see that each square (row) and its corresponding color is an __observations__. The __features__ in this case are the `size` and `edge` and the `color` is the __label__. In a __R__, the `data.frame()` function is used to depict the dataset above. 

```{r data, echo = TRUE, results = 'asis'}
squares <- data.frame(size = c("small", "big", "medium"),
                      edge = c("dotted", "stripped", "normal"),
                      color = c("green", "yellow", "green"))
print(xtable(squares, caption = "The dataset as a data.frame"), comment = FALSE)

```

The __observations__ correspond to the rows and the columns correspond to the __variables__. 

So the goal of Machine Learning, based on data shown in the example, is to build a __Model of Prediction__. Build a model that can help make predictions about the data for future instances of similar problems. But before the model can be built, one firstly has to acquaint themselves with the Data. The following Excercises deomstrate this process.

### Exercise 1: Getting acquanted with data 

As a first step, we will find out some properties of the dataset with which we will be working. More specifically, we want to know mre about the dataset's number of observations and variables. To do this, we will explore the `iris` dataset. 

#### Instructions:

* Use the two ways presented in the video to find out the number of observations and variables of the iris data set: `str()` and `dim()`. 
* Call `head()` and `tail()` on `iris` to reveal the first and last observations in the iris dataset. 
* Finally, call the `summary()` function to generate a summary of the dataset. What does the printout reveal?

#### Results:

```{r exercise1_1, echo= TRUE}
# The iris is available from the datasets package and is loaded by default
# Reveal number of observations and variables by looking at the structure
str(iris)

# Reveal number of observations and variables by looking at the dimensions
dim(iris)

# Show first and last observations in the iris data set
head(iris)
tail(iris)

# Summarize the iris data set
summary(iris)

```

### Exercise 2: Basic Prediction Model  

To examine a first take a using Machine Learning to make a prediction, we will be using the `Wage` dataset. This dataset contains the wage and some general informaiton for workers in the mid-Atlanic regions of the United States and there could be some relationship between the `age` of a worker and his/her `wage`. Older workers tend to earn more on average than their younger counterparts, hence one could expect an increasing trend in wage as workers age. So we build a linear regression model for you, using the `lm()` function to model the wage of a worker based on his/her age.

With a linear model `lm_wage`, that is built with previous observations, one can predict the wage of new observations. For example, suppose we want to predict the wage of a 60 year old worker. We can use the `predict()` function for this. This generic function takes a model as the first argument. The second argument should be some unseen observations as a data frame. The `predict()` function is then able to predict outcomes for these observations.

#### Instructions:

* Build a Linear Model called `lm_wage`, that models the `wage` by the `age` variable. 
* Create a single colum data frame called `unseen`, with a single column called `age`, containing a single value of `60`. 
* Predict the average wage at age 60 using the `predict()` function.

#### Results:

```{r exercise_2, echo = TRUE}
# The Wage dataset is already loaded from the outset.
# Build a Linear Model called lm_wage
lm_wage <- lm(wage ~ age, data = Wage)

# Create a data frame for an unseen age (60)
unseen <- data.frame(age = 60)

# Predict the wage of a 60 year old worker
result <- predict(lm_wage, unseen)

```

The Average wage of a 60 year old worker is __`r round(result, digits = 2)` USD__ per day. 

## Classification, Regression and Clustering  

In the majority of Machine Learning problems involve __Classification__, __Regression__ and __Clustering__. 

* A classification problem involves predicting whether a given observation belongs to a certain catagory. What's important to remember regaridng Classification problems, is that the output is __Qualitative__ and the possible classes to which a new observation can belong, are known beforehand (Predefined Classes). 
* A Regression problem involves predicting a continuous, quantitative value based on previous information. The input value are refered to as __Predictors__ and the outout is the __Response__. Regression is somehat simmilar to Classification in that it tries to estimate a funciton that maps the input to the output based on earlier observations, except that the estimate is an actual value. See [Appendix A: Regression Coefficients][]. What's important to remember is that the reponse is ALWAYS __Quantitative__ and the model can only be built with input knowledge of previous input/output observations. 
*  A Clustering problem involves tring to group `similar` observations into clusters, while making sure the clusters thmselves are `dissimilar`. Clustering is similar to Classification, but without saying to wich class the observations have  to belong, therfore no previous knowledge regaring the labels is required.

### Exercise 3: Classification for Filtering Spam

Filtering spam from relevant emails is a typical machine learning task. Information such as word frequency, character frequency and the amount of capital letters can indicate whether an email is spam or not. In the following exercise we work with the dataset `emails` from the [UCI Machine Learning Repository](http://s3.amazonaws.com/assets.datacamp.com/course/intro_to_ml/emails_small.csv). Here, several emails have been labeled by humans as spam (1) or not spam (0) and the results are found in the column `spam`. The considered feature in `emails` is `avg_capital_seq`. It is the average amount of sequential capital letters found in each email. Crete a very basic spamfilter, `spam_classifier()` that uses `avg_capital_seq` to predict whether an email is spam or not. Inspect the `emails` dataset, apply `spam_classifier()` to it and compare the outcome with the true labels. 

#### Instructions:

* Check the dimensions of this dataset, use `dim()`.
* Create the `spam_classifier()` function as a simple set of statements that decide between spam and no spam based on a single input vector.
* Pass the `avg_capital_seq` column of `emails` to `spam_classifier()` and thus determine which emails are spam and which aren't, assigning the resulting outcomes to `spam_pred`.
* Compare the prediction, `spam_pred`, to the true spam labels in `emails` and print out the result. How many of the emails were correctly classified?

#### Results:

```{r exercise_3, echo = TRUE}
# Load the emails dataset into the workspace
emails <- read.csv("data/emails_small.csv")

# Show the dimensions of emails
dim(emails)

# Inspect definition of spam_classifier()
spam_classifier <- function(x){
  prediction <- rep(NA,length(x))
  prediction[x > 4] <- 1
  prediction[x >= 3 & x <= 4] <- 0
  prediction[x >= 2.2 & x < 3] <- 1
  prediction[x >= 1.4 & x < 2.2] <- 0
  prediction[x > 1.25 & x < 1.4] <- 1
  prediction[x <= 1.25] <- 0
  return(prediction)
}

# Apply the classifier to the avg_capital_seq column as spam_pred
spam_pred <- spam_classifier(emails$avg_capital_seq)

# Compare spam_pred to emails$spam
spam_pred == emails$spam

```

### Exercise 4: LinkedIn Views for the next 3 days using Regression

Analyze the number of views of a LinkedIn profile by predicting how often a profile will be visited in the next 3 days, based on the views for the past 3 weeks. 

#### Instructions:

* Create a sample vector called `linkedin` with a random amount of profile visits.
* Create a vector `days` with the numbers from 1 to 21, which represent the previous 21 days of your linkedin views. 
* Fit a linear model that explains the `linkedin` views based on days. Use the `lm()` function with the appropriate formula. Assign the resulting linear model to `linkedin_lm`. 
* Using this linear model, predict the number of views for the next three days (22, 23 and 24). Use the `predict()` function and the predefined `future_days` data frame. Assign the result to `linkedin_pred`. 
* See how the remaining code plots both the historical data and the predictions. Try to interpret the result.

#### Results:

```{r exercise_4, echo = TRUE}
# Create the linkedin vector of the number of views per day for 3 weeks
linkedin <- c(5, 7, 4, 9, 11, 10, 14, 17, 13, 11, 18, 17, 21, 21, 24, 23, 28, 35, 21, 27, 23)

# Create the days vector
days <- seq(length(linkedin))

# Fit a linear model called on the linkedin views per day as linkedin_lm
linkedin_lm <- lm(linkedin ~ days)

# Predict the number of views for the next three days: linkedin_pred
future_days <- data.frame(days = 22:24)
linkedin_pred <- predict(linkedin_lm, future_days)

# Plot historical data and predictions
plot(linkedin ~ days, xlim = c(1, 24))
points(22:24, linkedin_pred, col = "green")

```

### Exercise 5: Separateing the Species of Iris using Clustering

This technique tries to group objects. It does this without any prior knowledge of what these groups could or should look like. In this case, the concepts of prior knowledge and unseen observations are less meaningful than for classification and regression. In this exercise, we group irises into 3 distinct clusters, based on several flower characteristics from the `iris` dataset. The clustering itself will be done with the `kmeans()` function.

__Note:__ In problems that have a random aspect (like `kmeans()`), the `set.seed()` function will be used to enforce reproducibility. If you fix the seed, the random numbers that are generated afterwards are always the same.

#### Instructions:

* Create a `data.frame` of Iris characteristics and vector of Iris species. 
* Use the `kmeans()` function. The first argument is `my_iris`; the second argument is `3`, as we want to find three groups in `my_iris`. Assign the result to a new variable `kmeans_iris`. 
* The actual species of the observations is stored in `species`. Use the `table()` function to compare it to the groups the clustering came up with. These groups can be found in the cluster attribute of `kmeans_iris`. 
* Generate a plot of `Petal.Length` against `Petal.Width` and `colors` by cluster.

#### Results:

```{r exercise_5, echo = TRUE}
# Set random seed.
set.seed(1)

# Create a data.frame of characteristics and vector of species
my_iris <- iris[-5]
species <- iris$Species

# Perform k-means clustering on my_iris as kmeans_iris
kmeans_iris <- kmeans(my_iris, 3)

# Compare the actual Species to the clustering using table()
table <- table(kmeans_iris$cluster)

# Plot Petal.Width against Petal.Length, coloring by cluster
plot(Petal.Length ~ Petal.Width, data = my_iris, col = kmeans_iris$cluster)

```

```{r echo = FALSE, results='asis'}
print(xtable(table, caption = "The clusters that kmeans() came up with"), comment = FALSE)

```

## Supervised vs. Unsupervised Machine Learning 

From the previous techniques, Classification and Regression, we note that are similarities. For both techniques, we try to find a function (or Model) that can later be used to predict labels (or values) for __unseen observations__. It is important that during the training of the function, labeled observations are available to the algorithm and are therefore examples of __Supervised Learning__ techniques. 

Since "labelling" can be tedeous work and is often done by humans, there are other techniques wich don't require labeled observations and are therefore called __Unsupervised Learning__ techniques. Clustering is an example of unsupervised learning in that it finds groups of observations that are similar without requiring a specific label for these observations. 

## Performance of the Model

In Supervised LEarning, the performance of a model can be determined by compaing the __real__ labels of observations with the __predicted__ labels. The goal is to have the models' predictions as close (or simialr) as possible to that of the real data. 

Aternatively, measuring the performance of Unsupervised Learning models is far more difficult, as we don;t have any real labels (or real observations) to compare with.

## Semi-supervised Learning

In Machine Learning, some techniques overlap between Supervised and Unsupervised learning. For example, our data may contain obersations that are `not` labeled and some that are. To address this kind of dataset:

1. Group similar observations together using __clustering__.
2. Use information about the cluster as well as the other labeled observations to assign a `class` to the unlabelled observations. 
3. This will provide more labeled observations to apply Supervised learning techniques on. 

### Exercise 6: Practical Supervised Learning

In the previous exercises, we used the `kmeans()` function to perform clustering on the `iris` dataset. Additionally, we created our own copy of the dataset by removing the `Species` attribute, thus we removed the labels of the observations. In this exercise, we will use the same dataset, but instead of dropping the `Species` labels, we will use them do some supervised learning. 

#### Instructions:

* Take a look at the `iris` dataset, using `str()` and `summary()`. 
* Create the code that builds a supervised learning model by using the `rpart()`. This model will train a decision tree on the `iris` dataset. 
* Use the `predict()` function with the tree model as the first argument. The second argument should be a dataframe containing observations of which we want to predict the label. In this case, we can use the predefined unseen data frame. The third argument should be `type = "class"`. 
* Simply print out the result of this prediction step.

```{r exercise_6, echo = TRUE}
# Set random seed. 
set.seed(1)

# Take a look at the iris dataset
#str(iris)
#summary(iris)

# Create decision tree model
tree <- rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, 
              data = iris, method = "class")

# Create A dataframe containing unseen observations
unseen <- data.frame(Sepal.Length = c(5.3, 7.2), 
                     Sepal.Width = c(2.9, 3.9), 
                     Petal.Length = c(1.7, 5.4), 
                     Petal.Width = c(0.8, 2.3))

# Predict the label of the unseen observations
results <- as.matrix(predict(tree, unseen, type = "class"))
```

#### Results:

```{r echo = FALSE, results='asis'}
# Print out the results
print(xtable(results, caption = "Resultant labels of Unseen Observations"), comment = FALSE)
```


### Exercise 7: How to do unsupervised lernin

In this exercise, we will group cars based on their horsepower and their weight by using the cars data frame, which has been derived from the `mtcars` dataset. To cluster the different observations in 2 groups , we will once again use `kmeans()`.

#### Instructions:

* Explore the dataset using `str()` and `summary()`. 
* Use `kmeans()` with two arguments to group the cars into two clusters based on the cars' __hp__ and __wt__, and assign the result to the `km_cars` variable. 
* Print out the cluster element of the `km_cars` variable to see which cars belong to which clusters.

```{r exercise_7, echo = TRUE}
# Create a subset of the mtcars datset with just wt and hp
cars <- subset(mtcars, select = c(wt, hp))

# Set random seed. Don't remove this line.
set.seed(1)

# Explore the cars dataset
#str(mtcars)
#summary(mtcars)

# Group the dataset into two clusters: km_cars
km_cars <- kmeans(cars, 2)

```

#### Results:

```{r echo = FALSE, results = 'asis'}
# Print out the contents of each cluster
results <- as.matrix(km_cars$cluster)
print(xtable(results, caption = "Output of the the two clusters of cars"), comment = FALSE)
```

As can be senn from the example, the __Ferrari Dino__ is in cluster __2__, while the __Fiat X1-9__ is grouped in cluster __1__. However, vizualizing the results, provides a more comprehensive overview.

### Exercise 8: Visualizing Unsupervised Learning Results

In the previous exercise, we grouped the cars based on their horsepower and their weight. An important part of machine learning is understanding the results of the model. In the case of clustering, visualization is key to interpretation! One way to achieve this, is by plotting the features of the cars and coloring the points based on their corresponding cluster.

In this exercise we summarize the reuslts of `kmeans()` in a comprehensive figure.

#### Instructions:

* Use the `plot()` command by coloring the cars based on their cluster. Do this by setting the col argument to the cluster partitioning vector: `km_cars$cluster.` 
* Print out the clusters' centroids, which are kind of like the centers of each cluster. They can be found in the centers __element__ of `km_cars`. 
* Create `points()` with the clusters's centroids. This will add the centroids to the earlier plot.

__Note:__ To learn about the other parameters that have been defined for you, have a look at the [__graphical parameters documentation__](http://www.rdocumentation.org/packages/graphics/functions/par). 

* If given a vector with a length equal to the number of plotted observations, the `col` argument will color each observation based on the corresponding element in the given vector. In this case, the coloring vector `km_cars$clusters` only contains 1's and 2's, which correspond to the colors "black" and "red" respectively. Hence, the objects in cluster one will be colored black, while those in cluster two will be colored red.
* The graphical parameter `pch` sets the points' symbol, in this case a filled square. 
* The graphical parameter `bg` sets the fill color of the points. Only applicable for pch symbols 21 through 25. 
* The graphical parameter `cex` sets the size of the points' symbol. In this case, it enlarges the symbol by 100%.

#### Results:

```{r exercise_8, echo = TRUE}
# Create a subset of the mtcars datset with just wt and hp
cars <- subset(mtcars, select = c(wt, hp))

# Set random seed. Don't remove this line
set.seed(1)

# Group the dataset into two clusters: km_cars
km_cars <- kmeans(cars, 2)

# Plot and color the points in the plot based on the clusters
plot(cars, col = km_cars$cluster)

# Print out the cluster centroids
km_cars$centers

# Add the centroids to the plot
points(km_cars$centers, pch = 22, bg = c(1, 2), cex = 2)

```

__The cluster centroids are typically good representations of all the observations in a cluster. They are often used to summarize clusters.__

\newpage

# Chapter 2: Measuring Performance (Measuring the Error)
## Introduction

How do we assess the quality of a particular Machine Learning model after ir has been "learned"? To answer this question, we have to note the following:

1. Define what is meant by Model quality or performance. The definition depends on the context on what the Model is being used. In most cases, this is described as:
- __Model Accurancy__
- __Computation Time__ 
- __Interpretability__ 
2. The different tasks of Machine Learning (Classification, Regression and Clustering), each have their own perfmance measures.

## Measuring Performance: Classification

In Classification systems, __Accurancy__ and __Error__ are the basic measures of performance as they reflect the number of times the system is right or wrong. Basically, the __Accuracy__ goes up when the __Error__ rate goes down, therefore:

$$Accuracy = \frac {Correctly \ Classified \ Instances}{Total \ Classified \ Instances}$$

__AND__

$$Error \ Rate = 1 - Accuracy$$

However, depending on the context of the Model it is always a good practice to calculate the __Confusion Matrix__. The Confusion Matrix consists of rows and colums, both contain all the available labels. Each cell contains the frequency of which the instances are classified in a particular way. __Figure 2__ shows an example of a __Confusion Matrix__.

\begin{figure}[h]
\center
\includegraphics{figures/confusionmatrix.png}
\caption{Example of a Confusion Matrix}
\label{Figure2}
\end{figure}

To uderstand how to interpret the Consuion Matrix, we will use a Binary Classifier example where one class is __positive__ and the other class is __negative__. 

#### True Positives

\begin{figure}[h]
\center
\includegraphics{figures/truepositive.png}
\caption{True Positives on the Confusion Matrix}
\label{Figure3}
\end{figure}

As Figure 3 shows, the upper left-hand corner of the Confusion Matris shows the frequency of __True Positives__. These are instances where the model has correctly classified as __Positive__. 

#### True Negatives

\begin{figure}[h]
\center
\includegraphics{figures/truenegative.png}
\caption{True Negatives on the Confusion Matrix}
\label{Figure4}
\end{figure}

Figure 4 shows the the lower right-hand corner of the Confusion Matrix representing __True Negatives__. This is the frequancy of the instances that are correctly classified, by the model, as being classified as __Negative__. 

#### False Negatives

Figure 5 illustrates the upper right-hand corner of the Confusion Matrix for __False Negatives__. These are the instances that are classified as __Negative__, but are actually __Potisitve__.

\begin{figure}[h]
\center
\includegraphics{figures/falsenegative.png}
\caption{False Negatives on the Confusion Matrix}
\label{Figure5}
\end{figure}

\newpage

#### False Positives

Figure 6 highlightes the lower left-hand corner of the Confusion Matrix, the frequency of __False Positives__. This is the frequancy of instances which are classified as __Positive__, but are actually __Negative__. 

\begin{figure}[h]
\center
\includegraphics{figures/falsepositive.png}
\caption{False Positives on the Confusion Matrix}
\label{Figure6}
\end{figure}

__Note:__ It is important to remember that when dealing with a Binary Classifier, `R` versions may display the possitive __positive__ and __negative__ in the reverse order to what is above figures show. Always pay attention to which colums and which rows show the predicted vs. true positive and negative respectivley. 

### Ratios in the Confusion Matrix

By using these above mentioned measures, we can now calculate new ratios:

- Accuracy
- Precision
- Recall

Where __Precision__ is:

$$Precision  = \frac{True \ Positives}{True \ Positives + False \ Positives}$$

And __Recall__ is:

$$Recall = \frac{True \ Positives}{True \ Positives + False \ Negatives}$$

Thus __Accuracy__ is:

$$Accuracy = \frac{\left( True \ Positives + True \ Negatives \right)}{\left( True \ Positives + False \ Positives + False \ Negatives + True \ Negatives \right)}$$

### Exercise 9: The Confusion Matrix

Have you ever wondered if you would have survived the Titanic disaster in 1912? [Kaggle](https://www.kaggle.com/c/titanic/data) has some historical data on this event. In this exercise a decision tree is learned on this dataset, that aims to predict whether a person would have survived the accident based on the variables `Age`, `Sex` and `Pclass` (travel class). The decision that the tree makes, can be deemed right or wrong if it knows what the true outcome was. Since the true fate of the passengers, `Survived`, is also provided in the Titanic data, we can compare it to the prediction of the tree. The results can be summarized in a Confusion Matrix. In `R`, we can use the `table()` function for this.

In this exercise, we will only focus on assessing the performance of the decision tree. In chapter 3, you will learn how to actually build a decision tree.

__Note:__ As in the previous chapter, there are functions that have a random aspect. The `set.seed()` function is used to enforce reproducibility. 

#### Instructions:

* Clean out the `titanic` data set to only show the `age`, `Sex` and `Pclass` labels. Review the structure of resultant data set `titanic` and infer the number of observations and variables? 
* Inspect the code that builds the decision tree, tree.
* Use the tree to `predict()` who survived in the titanic dataset. Use the tree as the first argument and titanic as the second argument. Make sure to set the type parameter to "class". Assign the result to `pred`.
* Build the Confusion Matrix with the `table()` function. This function builds a contingency table. The first argument corresponds to the rows in the matrix and should be the `Survived` column of `titanic` $\rightarrow$ the true labels. The second argument, corresponding to the columns, should be `pred` $\rightarrow$ the predicted labels.

```{r exercise_9, echo = TRUE}
# Load the titanic data set from Kaggle and extract only the needed variables
titanic <- read.csv("data/titanic.csv")
titanic <- subset(titanic, select = c(Survived, Pclass, Sex, Age))
titanic <- titanic[complete.cases(titanic), ] # Remove NA's

# Convert predictor to a factor
titanic$Survived <- as.factor(titanic$Survived)

# Set random seed. Don't remove this line
set.seed(1)

# Have a look at the structure of titanic
str(titanic)

# A decision tree classification model is built on the data
tree <- rpart(Survived ~ ., data = titanic, method = "class")

# Use the predict() method to make predictions, assign to pred
pred <- predict(tree, titanic, type = "class")

```

#### Results:

```{r echo = FALSE, results = 'asis'}
results <- as.matrix(table(titanic$Survived, pred))
print(xtable(results, caption = "Resultant Confusion Matrix"), comment = FALSE)

```

To summarize, __212__ out of all __265__ survivors were correctly predicted to have survived. On the other hand, __371__ out of the __449__ deceased were correctly predicted to have perished. Next we will put these numbers in more comprehensive ratios.

### Exercise 10: Deriving Ratios from the Confustion Matrix

The confusion matrix from the last exercise provides us with the raw performance of the decision tree: 

1. The survivors correctly predicted to have survived $\rightarrow$ `true positives` __(TP)__
2. The deceased who were wrongly predicted to have survived $\rightarrow$ `false positives` __(FP)__
3. The survivors who were wrongly predicted to have perished $\rightarrow$ `false negatives` __(FN)__
4. The deceased who were correctly predicted to have perished $\rightarrow$  `true negatives` __(TN)__

We've  seen that these values can be used to estimate comprehensive ratios to asses the performance of a classification algorithm. An example is the accuracy, which in this case represents the percentage of correctly predicted fates of the passengers.

$$Accuracy = \frac {TP + TN}{TP + FN + FP + TN}$$

Apart from accuracy, there is also precision and recall. Key metrics to assess the results of a classification algorithm:

$$Precision = \frac{TP}{TP + FP}$$

$$Recall = \frac{TP}{TP + FN}$$

#### Instructions:

* Assign the correct values of the confusion matrix to TP, FN, FP and TN. 
* Calculate the accuracy as acc and print it out. 
* Finally, also calculate the precision and the recall, as prec and rec. Print out both of them.

```{r exercise_10, echo = TRUE}
# Make the confusion matrix is available as conf
conf <- table(titanic$Survived, pred)

# Assign TP, FN, FP and TN using conf
TP <- conf[2, 2]
FN <- conf[2, 1]
FP <- conf[1, 2]
TN <- conf[1, 1]

# Calculate and print the accuracy: acc
acc <- (TP + TN)/(TP + FN + FP + TN)
print(acc)

# Calculate and print out the precision: prec
prec <- TP/(TP + FP)
print(prec)

# Calculate and print out the recall: rec
rec <- TP/(TP + FN)
print(rec)
```

#### Results:

With an accuracy of around __`r round(acc * 100)`%__, only __`r round((1 - acc) * 100)`%__ of the passengers were wrongly predicted to have survived or perished. This is mostly an acceptable, but not truly satisfying classifier. 

## Measuring Performance: Regression

To measure the perrformance of Regression algorithms, we use the __Root Mean Squared Error__ (RMSE). 

$$RMSE = \sqrt{\frac{1}{N} \sum_{i=1}^N \left( y_i - \hat y_i \right)^2}$$

$y_i$: Actual outcome for observation $i$  
$\hat y_i$: Predicted outcome for observation $i$  
$N$: Number of observations  

This equation shows the mean distance between each of the estimates and the regaression line, as shown in Figure 7. If these distances are small, the __RMSE__ will also be small. If the distances are large, then the __RMSE__ will also be large. 

\begin{figure}[h]
\center
\includegraphics{figures/rmse.png}
\caption{RMSE}
\label{Figure7}
\end{figure}

### Exercise 11: The Quality of a Regression

In this example we measure the sound pressure produced by an airplane's wing under different settings. These settings are the frequency of the wind, the angle of the wing, and several more. The results of this experiment are listed in the `air` dataset from [UCIMLR](https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise). We want to build a model that's able to predict the sound pressure based on these settings, instead of having to do those tedious experiments every time. 

In the exercise, there is an already prepared multivariable linear regression model, called `fit`. It takes as input the predictors: wind frequency (`freq`), wing's angle (`angle`) and chord's length (`ch_length`). The response is the sound pressure (`dec`). All these variables can be found in `air`.

The purpose of the exercise is to assess the quality of the model by calculating the root Mean Squared Error (RMSE).

#### Instructions:

* Take a look at the structure of air. What does it show?
* Inspect the code that builds a multivariable liner regression model based on air. 
* Use the `predict()` function to make predictions for the observations in the `air` dataset. Simply pass `fit` to `predict()`. 
* Assign the result to `pred`. 
* Calculate the __RMSE__ using the mentioned formula above. $y_i$ corresponds to the actual sound pressure of observation $i$, which is in `air$dec`. $\hat y_i$ corresponds to the predicted value of observation $i$, which is in `pred`. Assign the resulting RMSE to `rmse`. 
* Print out `rmse`.

```{r exercise_11, echo = TRUE}
# Load the air dataset: air
air <- read.table("data/airfoil_self_noise.dat")
colnames(air) <- c("freq", "angle", "ch_length", "velocity",
                   "thickness", "dec")

# Take a look at the structure of air
str(air)

# Inspect your colleague's code to build the model
fit <- lm(dec ~ freq + angle + ch_length, data = air)

# Use the model to predict for all values: pred
pred <- predict(fit)

# Use air$dec and pred to calculate the RMSE 
N <- nrow(air)
rmse <- sqrt(sum((air$dec - pred)^2)/N)
```

#### Results:

The RMSE is given by __`r round(rmse)`__. But what exactly does __`r round(rmse)`__ actually mean? 

This is __`r round(rmse)`__ decibels, the unit of the sound pressure. As a standalone number, it means nothing. In order to derive its meaning, it should be compared to the RMSE of a different model for the same problem, as the next exercise will show. 

### Exercise 12: Adding complexity to increase quality

In the last exercise, the model had 3 predictors or input variables, but what happens if we included more predictors? Since the data includes measurements on free-stream velocity $\rightarrow$ `velocity` and suction side displacement thickness $\rightarrow$ thickness, available for use in the `air` dataset as well. Adding the new variables will definitely increase the complexity of the model, but will it increase the performance? A first step to should take is once again calculate the RMSE and compare it to the previous result.

#### Instructions:

* Use the `predict()` function to make predictions using `fit2`, for __all__ values in the `air` dataset. Assign the resultin vector to `pred2`. 
* Calculate the __RMSE__ using the formula above. Assign this value to `rmse2`. 
* Print `rmse2` and compare it with the earlier `rmse`. What can we conclude?

```{r exercise_12, echo = TRUE}
# Use he air dataset from the previous exercise
# re-fit the previous model
fit <- lm(dec ~ freq + angle + ch_length, data = air)
pred <- predict(fit)
rmse <- sqrt(sum( (air$dec - pred) ^ 2) / nrow(air))
rmse

# Your colleague's more complex model
fit2 <- lm(dec ~ freq + angle + ch_length + velocity + thickness, data = air)

# Use the model to predict for all values: pred2
pred2 <- predict(fit2)

# Calculate rmse2
rmse2 <- sqrt(sum( (air$dec - pred2)^2 / nrow(air)))
```

#### Results:

Adding complexity seems to have caused the __RMSE__ to decrease, from __`r round(rmse, digits = 3)`__ to __`r round(rmse2, digits = 3)`__. But there's more going on here; maybe adding variables always leads to a decrease of the RMSE?

## Measuring Performance: Clustering

Measuring the performance of a Clustering algorithm is somewhat difficult because there is no label information. So the best way to measure how well it performed is to measure the distance between the various points. Specifically, the performance measure for Clustering is comprised of two elements:

1. The similarity __within__ each cluster $\rightarrow$ How alike are same points within the cluster? To measure this, we minimize the following: 
    - Within Cluster Sum of Squares (WSS)
    - Diameter
2. The similarity __between__ the various clusters $\rightarrow$ How alike are the points between the various different clusters? To measure this, we maximize the following: 
    - Between Cluster sum of Squares (BSS)
    - Intercluster distance  

__Note:__ For a well performaning algorithm, we want the first metric (similarity within the cluster) to be __high__ and the second metric (similarity between clusters), to be __low__.

3. Dunn's Index

$$Dunn's \ Index = \frac{Minimal \ Intercluster \ Distance}{Maximal \ Diameter}$$

### Exercise 13: Clustering

In the dataset `seeds` we can find various metrics for 210 seeds, such as `area`, `perimeter` and `compactness`, among others from [UCIMLR](https://archive.ics.uci.edu/ml/datasets/seeds). However, the seeds' labels are lost. Hence, we don't know which metrics belong to which type of seed. What we do know, is that there were three types of seeds.

The code on the below creates clusters of the seeds into three clusters, called `km_seeds`, but is it likely that these three clusters represent our seed types?

There are two initial steps to potentially take: 

1. Visualize the resulting clusters for two variables, for example `length` versus `compactness`. 
2. Verify if the clusters are well separated and compact. To this end, we can calculate the __between__ and __within__ cluster sum of squares respectively.

#### Instructions:

* Look at the structure of the seeds dataset. 
* Extend the `plot()` command by coloring the observations based on their cluster. Do this by setting the `col` argument equal to the cluster element of `km_seeds`. 
* Print out the ratio of the within sum of squares to the between cluster sum of squares, so __WSS/BSS__. These measures can be found in the cluster object `km_seeds` as `tot.withinss` and `betweenss`. Is the within sum of squares substantionally lower than the between sum of squares?

```{r exercise_13, echo = TRUE}
# The seeds dataset is already loaded into your workspace
seeds <- read.table("data/seeds_dataset.txt")
colnames(seeds) <- c("ares", "perimeter", "compactness", "length",
                     "width", "asymmetry", "groove_length")
seeds <- seeds[, 1:7]

# Set random seed. Don't remove this line
set.seed(1)

# Explore the structure of the dataset
str(seeds)

# Group the seeds in three clusters
km_seeds <- kmeans(seeds, 3)

# Color the points in the plot based on the clusters
plot(length ~ compactness, data = seeds, col = km_seeds$cluster)

# Print out the ratio of the WSS to the BSS
print(km_seeds$tot.withinss/km_seeds$betweenss)

```

#### Results: 

The within sum of squares __(`r round(km_seeds$tot.withinss, digits = 2)`)__ is far lower than the between sum of squares __(`r round(km_seeds$betweenss, digits = 2)`)__. Indicating the clusters are well seperated and overall compact. This is further strengthened by the plot above, where the clusters were visually distinct for these two variables. It's likely that these three clusters represent the three seed types well, even if there's no way to truly verify this. 

## The Training Set and the Test Set  

The maine difference between the classical statistics and Supervised Learning/Machine Leaning is all about the __Predictive Power vs. Desciptive Power__. With Supervised Learning, it is essential that the leaned model must have a strong predictive power. If a model has a strong predictive power, it is able to make good precictions about new __unseen__ observations. Classical statistics on the other hand is typically more concerned about how well the model fits the complete data set $\rightarrow$ to explain or describe the data. 

When learning a __predictive__ model, typically a complete data set is __NOT__ used to train the model. The data set is usually broken down into two specifica parts:

- __Training Set__ $\rightarrow$ Used for `training` the model.
- __Test Set__ $\rightarrow$ Used to evluate the actual performance if the model on `unseen` data. 

It is very importnt to realize that the Training Set and the Test Set are __disjoint__ sets. In other words, they don't have any observations in common $\rightarrow$ NO OVERLAP. This clear separation makes it possible to test wether the trained model __generalizes__ well to unseen data. 

For example, take Classification problem with a data set of $N$ observations ($X$), $K$ features ($F$) and $y$ class labels for each observation, as is illustrated int Figure 8. The resultant Confusion Matrix provides us with a clear idea of the actual predictive performance of the Classifier. 

\begin{figure}[h]
\center
\includegraphics{figures/split.png}
\caption{Splitting the Data}
\label{Figure8}
\end{figure}

 __Note:__ The concepts of Train and Test sets are only important inthe context of Supervised Learning. these concepts are NOT used in Unserpervised Lening because the data is not labeled. 

### Creating the Training and Test Sets

In order to get the most benefit of traing and testing the model, there are a number of  key considerations to take note of when determiningwhich instances of the observations go in the Training Set and which instances go in the Test Set.

Firstly, one typically chooses a Training Set that is larger then the Test Set to train the model on the majority of the obersations to improve the model's accurancy (more data = better model) and most cases this is a ratio of __3:1__. But one should take care that the Test Set is not too small as the model performance measures should be indiciative of the overall performance of the model when used with further unseen observations. 

Secondly, depending on the type of Machine Learning model being used, there are a number of factors that influence which elements are added to the Training Set and which are allocated to the Test Set.

- __Classification__ $\rightarrow$ Classes within the Test Set and the Training Set should have similar distributions. This ensures that all classes are available in each of the sets. 
- __Classification and Regression__ $\rightarrow$ Shuffle the complete data set before splitting them. 

Finally, sampling the data in a particular way to create the Test Set can impact the performancce measures on the Set. To strengthen the measure, we should make use of __cross-validation__. This means that we use a learning algorithm to train a model multiple times, each time, with a different separation of Traing and Test data. Figure 9 illustrates this, where the data is "foled" four times, with each fold producing a differnt set of Training and Test data. The results from each fold are aggregated to form a robust measure. 

\begin{figure}[h]
\center
\includegraphics{figures/4fold.png}
\caption{4-fold Cross Validation}
\label{Figure9}
\end{figure}

### Exercise 14: Splitting the Sets

In this exercise we will be ssing the `titanic` dataset from [Exercise 9](Exercise 9: The Confusion Matrix), where we set up a decision tree and assessed its performance by calculating the confusion matrix. However, the tree was built using the entire set of observations, so the resultant confusion matrix doesn't assess the predictive power of the `tree`. The training set and the test set were one and the same thing: this can be done better!

First off, we'll want to split the dataset into a Training Set and a Test Set. But before that, notice that the titanic dataset is sorted on `titanic$Survived` , so the dataset must be shuffled first in order to have a fair distribution of the output variable in both sets. 

For example, the following commands could to shuffle a data frame `df` and divide it in a training and test set with a `60/40` split.

```{r eval = FALSE}
n <- nrow(df)
shuffled_df <- df[sample(n), ]
train_indices <- 1:round(0.6 * n)
train <- shuffled_df[train_indices, ]
test_indices <- (round(0.6 * n) + 1):n
test <- shuffled_df[test_indices, ]

```

__Note:__ The example shows how to do a `60/40 split`. In the exercise a 70/30 split should be done. 

#### Instructions:

* Shuffle the observations of the titanic dataset and store the result in shuffled. 
* Split the dataset into a train set, and a test set. Use a `70/30` split. 
* Print out the structure of both train and test with `str()`. Does your result make sense?

```{r exercise_14, echo = TRUE}
# Create a fresh load the titanic dataset
titanic <- read.csv("data/titanic.csv")
titanic <- subset(titanic, select = c(Survived, Pclass, Sex, Age))
titanic <- titanic[complete.cases(titanic), ] # Remove NA's
titanic <- titanic[with(titanic, order(titanic$Survived)),] # Order by Survived

# Set random seed. Don't remove this line.
set.seed(1)

# Shuffle the dataset, call the result shuffled
n <- nrow(titanic)
shuffled <- titanic[sample(n), ]

# Split the data in train and test
train_index <- 1:round(0.7 * n)
test_index <- (round(0.7 * n) + 1):n
train <- shuffled[train_index, ]
test <- shuffled[test_index, ]

```

#### Results:  

```{r echo = FALSE}
# Print the structure of train and test
str(train)
str(test)

```

### Exercise 15: Training then Testing

In this exercise we redo the model used in [Exercise 9](Exercise 9: The Confusion Matrix), this time, however, we build a decision tree on the training set and then assess its predictive power on the test set. 

#### Instructions: 

* Correct the decision tree model that was learned to be learned from the `training` set.
* Use the `predict()` function with the first argument being the tree model and the second argument being the correct data set. Set the `type` to "class" and call the predicted vector `pred`.
* Use the `table()` function to calculate the confusion matrix. Assign the table to `conf`. Remember that the actual labels of the test set should be the rows and the predicted labels, the columns.
* Print out `conf`.

```{r exercise_15, echo = TRUE}
# Create a fresh load the titanic dataset
titanic <- read.csv("data/titanic.csv")
titanic <- subset(titanic, select = c(Survived, Pclass, Sex, Age))
titanic <- titanic[complete.cases(titanic), ] # Remove NA's
titanic <- titanic[with(titanic, order(titanic$Survived)),] # Order by Survived

# Set random seed. Don't remove this line.
set.seed(1)

# Shuffle the dataset; build train and test
n <- nrow(titanic)
shuffled <- titanic[sample(n),]
train <- shuffled[1:round(0.7 * n),]
test <- shuffled[(round(0.7 * n) + 1):n,]

# Change the model that has been learned. One of the arguments is incorrect.
tree <- rpart(Survived ~ ., train, method = "class")

# Predict the outcome on the test set with tree: pred
pred <- predict(tree, test, type = "class")

# Calculate the confusion matrix
conf <- table(test$Survived, pred)

```

#### Results: 

```{r echo = FALSE, results = 'asis'}
# Print this confusion matrix
results <- as.matrix(conf)
print(xtable(results, caption = "Resultant Confusion Matrix"), comment = FALSE)

```

The confusion matrix reveals an accuracy of __`r round( (conf[2, 2] + conf[1, 1]) / (conf[2, 2] + conf[2, 1] + conf[1, 2] + conf[1, 1]), digits = 2) * 100`%__, which is less than the __82%__ calculated in [Exercise 9](Exercise 9: The Confusion Matrix). However, this is a much more trustworthy estimate of the model's true predictive power.

### Exercise 16: Manual implementation of Cross Validation

In this exercise we will evaluate getting better predictive performance using __Cross Validation__, by "folding" the dataset 6 times and calculating the accuracy for each fold. The mean of these accuracies forms a more robust estimation of the real accuracy on unseen data, as it depends less on the choice of training and test set.

__Note:__ Other performance measures, such as recall or precision, can be used as well. 

#### Instructions:

* Initialize a vector of length 6. The elements should all be `0`. 
* Split the dataset correctly 6 times and build a model on the training set inside a for loop. 
* Use the model to predict the values of the test set. Use `predict()` and set `type` to "class". Assign the result to `pred`. 
* Make the confusion matrix using `table()` and assign it to `conf`. 
* Assign the accuracy of the model to the correct index in `accs`. Use the confusion matrix, `conf`.
* Finally, print the mean accuracy. 

Tip: `diag()` provides the elements on the diagonal of a matrix. 

```{r exercise_16, echo = TRUE}
# Uee the shuffled dataset from the previous exercise.
shuffled$Survived <- as.factor(shuffled$Survived)

# Set random seed. Don't remove this line.
set.seed(1)

# Initialize the accs vector
accs <- seq(0, 0, length = 6)

for (i in 1:6) {
  # These indices indicate the interval of the test set.
  # For example, if i = 1, then the indices will be from 1 to 119,
  # and if i = 2, then the indices will be from 120 to 238 etc.
  indices <- (((i-1) * round((1/6)*nrow(shuffled))) + 1):
          ((i*round((1/6) * nrow(shuffled))))
  
  # Exclude them from the train set using i set of indices
  train <- shuffled[-indices,]
  
  # Include them in the test set usinf i set of indices
  test <- shuffled[indices,]
  
  # A model is learned using each training set: tree
  tree <- rpart(Survived ~ ., train, method = "class")
  
  # Make a prediction on the test set using tree: pred
  pred <- predict(tree, test, type = "class")
  
  # Assign the confusion matrix: conf
  conf <- table(test$Survived, pred)
  
  # Assign the accuracy of this model to the ith index in accs
  accs[i] <- sum(diag(conf)) / sum(conf)
}
```

#### Results:

```{r echo = TRUE}
# Print out the mean of accs
print(mean(accs))

```

As can be seen, the resultant estimate __(`r round(mean(accs) * 100)`%)__ is  a more robust measure of the accuracy. It will also be less susceptible to the randomness of splitting the dataset. 


## Bias and Variance

Since __Prediction__ is the main goal behind Supervised Learning techniques, the __Prediction Error__ of a model can be split into components:

1. Reducible Error
2. Irreducible Error

The __Irreducible Error__ can be seen as noise or irregularities in the data and should NOT be minimized when training a model. Of more importance is the __Reducible Error__. This is the error due to an "unfit" model and the lower this error, the better the learning algorithm. The Reducible Error can be split into an error due to __Bian__ and an error due to __Variance__. __Bias__ and __Variance__ are two essential elements in Machine Learning as they highlight the key concepts behind why it can be a challenge. 

The error due to __Bias__ is derived from assumptions that are made when specifing the learning algorithm. Choosing the incorrect model will cause the error due to bias to be __high__ because the model is being restricted and unable to fit the data more effectivley (the more restrictions, the higher the bias). So in essence, the error due to __Bias__ is the difference between predictions made by the chosen learning algorithm and the true values. 

The error due to __Variance__ is the prediction error that is caused by the sampling of the dataset, specifically the training set. A model with a high variance fits the training set closely including the "noise". Having the model fit the trainign set too closely will not generalize well to the test set (or new data). 

To this end there needs to be a __trade-off__ between Bias and Variance. A low Bias, not resticting the learning model and allowing it to fit perfectly, leads to high Variance. On the contrary, when the Bias is high, then model is more restricted and variance is lower. This is because it is less sensitive to the changes in the traing set. 

__Figure 10__ provides a gopod visual overview of the __Bias vs. Variance__ trade-off.

\begin{figure}[h]
\center
\includegraphics{figures/biasvariance.png}
\caption{Bias vs. Variance Trade-off}
\label{Figure10}
\end{figure}

## Overfitting and Underfitting

__Overfitting__ and __Underfitting__ are another set of terms often used when disussing the accuracy of a particular model. 

When splitting the data, the accuracy of a model with __Variance__ will depend on which data is in the __training__ set. Basically the model fits the training set far too well (high variance) $\rightarrow$ __Overfitting__. In other words, the model prefectly describes the training set, but generalizes badly to the test set (too sepcific to the trainig set). 

__Underfitting__ means that the the model is restricted, it's __Bias__ is too high or too general. 

### Exercise 17: Overfitting

In this exercise, we will make use of the crude spamfilter, `spam_classifier()`, from chapter 1. Recall that it filters spam based on the average sequential use of capital letters (`avg_capital_seq`) to decide whether an email was spam (`1`) or not (`0`). However the set, `emails_small` that was used to test the classifier was only a small fraction of the entire dataset. In this example, we make use of the entire data set, `emails_full` from [UCIMLR](https://archive.ics.uci.edu/ml/datasets/Spambase).

the objective of this execersie is to verify whether the `spam_classifier` generalizes to the entire set of emails. The accuracy for the set, `emails_small` is equal to 1. Is the accuracy for the entire set `emails_full` substantially lower?

#### Instructions

* Apply `spam_classifier()` on the `avg_capital_seq` variable in `emails_full` and save the results in `pred_full`. 
* Create a confusion matrix, using `table()`: `conf_full`. Put the true labels found in `emails_full$spam` in the rows. 
* Use `conf_full` to calculate the accuracy: `acc_full`. 
* Print out the result. 

Tip: The functions `diag()` and `sum()` will help. 

```{r exercise_17, echo = TRUE}
# The spam filter that has been 'learned'
spam_classifier <- function(x){
  prediction <- rep(NA,length(x))
  prediction[x > 4] <- 1
  prediction[x >= 3 & x <= 4] <- 0
  prediction[x >= 2.2 & x < 3] <- 1
  prediction[x >= 1.4 & x < 2.2] <- 0
  prediction[x > 1.25 & x < 1.4] <- 1
  prediction[x <= 1.25] <- 0
  return(factor(prediction, levels=c("1","0")))
}

# Apply spam_classifier to emails_full: pred_full


# Build confusion matrix for emails_full: conf_full


# Calculate the accuracy with conf_full: acc_full


# Print acc_full
```

\newpage

# Appendix A: Regression Coefficients

As an example of trying to fit a linear function between a `Predictor` (e.g. Weight) and a `Response` (e.g. Height). $$Height \approx \beta_0+\beta_1\times Weight$$ Together $\beta_1$ and $\beta_0$ are known as the model coefficients or parameters. As soon as we know what these coefficients are, the function is able to convert any new input to output. This means that solving the Machine Learning problem is to find good values for $\beta_1$ and $\beta_0$, estimated from previous input to previous output observations. 
