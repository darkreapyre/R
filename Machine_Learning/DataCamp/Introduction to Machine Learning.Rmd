---
title: "Introduction to Machine Learning"
output: 
    pdf_document:
        toc: true
    fontsize: 11pt
    geometry: margin=1in
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
# make this an external chunk that can be included in any file
require(knitr)
require(xtable)
require(ISLR)
require(rpart)
options(xtable.comment = TRUE)
#opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

#Add Table Captions
#table <- [OUTPUT]
#```{r echo = FALSE, results='asis'}
#print(xtable(table, caption = "caption"), comment = FALSE)
#```

```

\newpage
 
# Chapter 1: What is Machine Learning   
## Introduction  

__Machine Learnng:__ 

* Explores the construction and usage of algorithms. 
* Improves performance as it receives __more__ infomrtion. 
* Experience comes from observations on how particular problems have been previously solved.

No matter what algorithm used, the primary concept for Machine Learning is __imput knowledge__ or __Data__. Typically this data is a __dataset__ containing a number of observations, each having a number of well defined variables (often called features) :

\begin{figure}[h]
\center
\includegraphics{figures/data.png}
\caption{Example of some Data}
\label{Figure1}
\end{figure}

From __Figure 1__, we see that each square (row) and its corresponding color is an __observations__. The __features__ in this case are the `size` and `edge` and the `color` is the __label__. In a __R__, the `data.frame()` function is used to depict the dataset above. 

```{r data, echo = TRUE, results = 'asis'}
squares <- data.frame(size = c("small", "big", "medium"),
                      edge = c("dotted", "stripped", "normal"),
                      color = c("green", "yellow", "green"))
print(xtable(squares, caption = "The dataset as a data.rame"), comment = FALSE)

```

The __observations__ correspond to the rows and the columns correspond to the __variables__. 

So the goal of Machine Learning, based on data shown in the example, is to build a __Model of Prediction__. Build a model that can help make predictions about the data for future instances of similar problems. But before the model can be built, one firstly has to acquaint themselves with the Data. The following Excercises deomstrate this process.

### Exercise 1: Getting acquanted with data 

As a first step, we will find out some properties of the dataset with which we will be working. More specifically, we want to know mre about the dataset's number of observations and variables. To do this, we will explore the `iris` dataset. 

#### Instructions:

* Use the two ways presented in the video to find out the number of observations and variables of the iris data set: `str()` and `dim()`. 
* Call `head()` and `tail()` on `iris` to reveal the first and last observations in the iris dataset. 
* Finally, call the `summary()` function to generate a summary of the dataset. What does the printout reveal?

#### Results:

```{r exercise1_1, echo= TRUE}
# The iris is available from the datasets package and is loaded by default
# Reveal number of observations and variables by looking at the structure
str(iris)

# Reveal number of observations and variables by looking at the dimensions
dim(iris)

# Show first and last observations in the iris data set
head(iris)
tail(iris)

# Summarize the iris data set
summary(iris)

```

### Exercise 2: Basic Prediction Model  

To examine a first take a using Machine Learning to make a prediction, we will be using the `Wage` dataset. This dataset contains the wage and some general informaiton for workers in the mid-Atlanic regions of the United States and there could be some relationship between the `age` of a worker and his/her `wage`. Older workers tend to earn more on average than their younger counterparts, hence one could expect an increasing trend in wage as workers age. So we build a linear regression model for you, using the `lm()` function to model the wage of a worker based on his/her age.

With a linear model `lm_wage`, that is built with previous observations, one can predict the wage of new observations. For example, suppose we want to predict the wage of a 60 year old worker. We can use the `predict()` function for this. This generic function takes a model as the first argument. The second argument should be some unseen observations as a data frame. The `predict()` function is then able to predict outcomes for these observations.

#### Instructions:

* Build a Linear Model called `lm_wage`, that models the `wage` by the `age` variable. 
* Create a single colum data frame called `unseen`, with a single column called `age`, containing a single value of `60`. 
* Predict the average wage at age 60 using the `predict()` function.

#### Results:

```{r exercise1_2, echo = TRUE}
# The Wage dataset is already loaded from the outset.
# Build a Linear Model called lm_wage
lm_wage <- lm(wage ~ age, data = Wage)

# Create a data frame for an unseen age (60)
unseen <- data.frame(age = 60)

# Predict the wage of a 60 year old worker
result <- predict(lm_wage, unseen)

```

The Average wage of a 60 year old worker is __`r round(result, digits = 2)` USD__ per day. 

## Classification, Regression and Clustering  

In the majority of Machine Learning problems involve __Classification__, __Regression__ and __Clustering__. 

* A classification problem involves predicting whether a given observation belongs to a certain catagory. What's important to remember regaridng Classification problems, is that the output is __Qualitative__ and the possible classes to which a new observation can belong, are known beforehand (Predefined Classes). 
* A Regression problem involves predicting a continuous, quantitative value based on previous information. The input value are refered to as __Predictors__ and the outout is the __Response__. Regression is somehat simmilar to Classification in that it tries to estimate a funciton that maps the input to the output based on earlier observations, except that the estimate is an actual value. See [Appendix A: Regression Coefficients][]. What's important to remember is that the reponse is ALWAYS __Quantitative__ and the model can only be built with input knowledge of previous input/output observations. 
*  A Clustering problem involves tring to group `similar` observations into clusters, while making sure the clusters thmselves are `dissimilar`. Clustering is similar to Classification, but without saying to wich class the observations have  to belong, therfore no previous knowledge regaring the labels is required.

### Exercise 3: Classification for Filtering Spam

Filtering spam from relevant emails is a typical machine learning task. Information such as word frequency, character frequency and the amount of capital letters can indicate whether an email is spam or not. In the following exercise we work with the dataset `emails` from the [UCI Machine Learning Repository](http://s3.amazonaws.com/assets.datacamp.com/course/intro_to_ml/emails_small.csv). Here, several emails have been labeled by humans as spam (1) or not spam (0) and the results are found in the column `spam`. The considered feature in `emails` is `avg_capital_seq`. It is the average amount of sequential capital letters found in each email. Crete a very basic spamfilter, `spam_classifier()` that uses `avg_capital_seq` to predict whether an email is spam or not. Inspect the `emails` dataset, apply `spam_classifier()` to it and compare the outcome with the true labels. 

#### Instructions:

* Check the dimensions of this dataset, use `dim()`.
* Create the `spam_classifier()` function as a simple set of statements that decide between spam and no spam based on a single input vector.
* Pass the `avg_capital_seq` column of `emails` to `spam_classifier()` and thus determine which emails are spam and which aren't, assigning the resulting outcomes to `spam_pred`.
* Compare the prediction, `spam_pred`, to the true spam labels in `emails` and print out the result. How many of the emails were correctly classified?

#### Results:

```{r exercise1_3, echo = TRUE}
# Load the emails dataset into the workspace
emails <- read.csv("data/emails_small.csv")

# Show the dimensions of emails
dim(emails)

# Inspect definition of spam_classifier()
spam_classifier <- function(x){
  prediction <- rep(NA,length(x))
  prediction[x > 4] <- 1
  prediction[x >= 3 & x <= 4] <- 0
  prediction[x >= 2.2 & x < 3] <- 1
  prediction[x >= 1.4 & x < 2.2] <- 0
  prediction[x > 1.25 & x < 1.4] <- 1
  prediction[x <= 1.25] <- 0
  return(prediction)
}

# Apply the classifier to the avg_capital_seq column as spam_pred
spam_pred <- spam_classifier(emails$avg_capital_seq)

# Compare spam_pred to emails$spam
spam_pred == emails$spam

```

### Exercise 4: LinkedIn Views for the next 3 days using Regression

Analyze the number of views of a LinkedIn profile by predicting how often a profile will be visited in the next 3 days, based on the views for the past 3 weeks. 

#### Instructions:

* Create a sample vector called `linkedin` with a random amount of profile visits.
* Create a vector `days` with the numbers from 1 to 21, which represent the previous 21 days of your linkedin views. 
* Fit a linear model that explains the `linkedin` views based on days. Use the `lm()` function with the appropriate formula. Assign the resulting linear model to `linkedin_lm`. 
* Using this linear model, predict the number of views for the next three days (22, 23 and 24). Use the `predict()` function and the predefined `future_days` data frame. Assign the result to `linkedin_pred`. 
* See how the remaining code plots both the historical data and the predictions. Try to interpret the result.

#### Results:

```{r exercise1_4, echo = TRUE}
# Create the linkedin vector of the number of views per day for 3 weeks
linkedin <- c(5, 7, 4, 9, 11, 10, 14, 17, 13, 11, 18, 17, 21, 21, 24, 23, 28, 35, 21, 27, 23)

# Create the days vector
days <- seq(length(linkedin))

# Fit a linear model called on the linkedin views per day as linkedin_lm
linkedin_lm <- lm(linkedin ~ days)

# Predict the number of views for the next three days: linkedin_pred
future_days <- data.frame(days = 22:24)
linkedin_pred <- predict(linkedin_lm, future_days)

# Plot historical data and predictions
plot(linkedin ~ days, xlim = c(1, 24))
points(22:24, linkedin_pred, col = "green")

```

### Exercise 5: Separateing the Species of Iris using Clustering

This technique tries to group objects. It does this without any prior knowledge of what these groups could or should look like. In this case, the concepts of prior knowledge and unseen observations are less meaningful than for classification and regression. In this exercise, we group irises into 3 distinct clusters, based on several flower characteristics from the `iris` dataset. The clustering itself will be done with the `kmeans()` function.

__Note:__ In problems that have a random aspect (like `kmeans()`), the `set.seed()` function will be used to enforce reproducibility. If you fix the seed, the random numbers that are generated afterwards are always the same.

#### Instructions:

* Create a `data.frame` of Iris characteristics and vector of Iris species. 
* Use the `kmeans()` function. The first argument is `my_iris`; the second argument is `3`, as we want to find three groups in `my_iris`. Assign the result to a new variable `kmeans_iris`. 
* The actual species of the observations is stored in `species`. Use the `table()` function to compare it to the groups the clustering came up with. These groups can be found in the cluster attribute of `kmeans_iris`. 
* Generate a plot of `Petal.Length` against `Petal.Width` and `colors` by cluster.

#### Results:

```{r exercise1_5, echo = TRUE}
# Set random seed.
set.seed(1)

# Create a data.frame of characteristics and vector of species
my_iris <- iris[-5]
species <- iris$Species

# Perform k-means clustering on my_iris as kmeans_iris
kmeans_iris <- kmeans(my_iris, 3)

# Compare the actual Species to the clustering using table()
table <- table(kmeans_iris$cluster)

# Plot Petal.Width against Petal.Length, coloring by cluster
plot(Petal.Length ~ Petal.Width, data = my_iris, col = kmeans_iris$cluster)

```

```{r echo = FALSE, results='asis'}
print(xtable(table, caption = "The clusters that kmeans() came up with"), comment = FALSE)

```

## Supervised vs. Unsupervised Machine Learning 

From the previous techniques, Classification and Regression, we note that are similarities. For both techniques, we try to find a function (or Model) that can later be used to predict labels (or values) for __unseen observations__. It is important that during the training of the function, labeled observations are available to the algorithm and are therefore examples of __Supervised Learning__ techniques. 

Since "labelling" can be tedeous work and is often done by humans, there are other techniques wich don't require labeled observations and are therefore called __Unsupervised Learning__ techniques. Clustering is an example of unsupervised learning in that it finds groups of observations that are similar without requiring a specific label for these observations. 

## Performance of the Model

In Supervised LEarning, the performance of a model can be determined by compaing the __real__ labels of observations with the __predicted__ labels. The goal is to have the models' predictions as close (or simialr) as possible to that of the real data. 

Aternatively, measuring the performance of Unsupervised Learning models is far more difficult, as we don;t have any real labels (or real observations) to compare with.

## Semi-supervised Learning

In Machine Learning, some techniques overlap between Supervised and Unsupervised learning. For example, our data may contain obersations that are `not` labeled and some that are. To address this kind of dataset:

1. Group similar observations together using __clustering__.
2. Use information about the cluster as well as the other labeled observations to assign a `class` to the unlabelled observations. 
3. This will provide more labeled observations to apply Supervised learning techniques on. 

### Exercise 6: Practical Supervised Learning

In the previous exercises, we used the `kmeans()` function to perform clustering on the `iris` dataset. Additionally, we created our own copy of the dataset by removing the `Species` attribute, thus we removed the labels of the observations. In this exercise, we will use the same dataset, but instead of dropping the `Species` labels, we will use them do some supervised learning. 

#### Instructions:

* Take a look at the `iris` dataset, using `str()` and `summary()`. 
* Create the code that builds a supervised learning model by using the `rpart()`. This model will train a decision tree on the `iris` dataset. 
* Use the `predict()` function with the tree model as the first argument. The second argument should be a dataframe containing observations of which we want to predict the label. In this case, we can use the predefined unseen data frame. The third argument should be `type = "class"`. 
* Simply print out the result of this prediction step.

```{r exercise 1_6, echo = TRUE}
# Set random seed. 
set.seed(1)

# Take a look at the iris dataset
#str(iris)
#summary(iris)

# Create decision tree model
tree <- rpart(Species ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width, 
              data = iris, method = "class")

# Create A dataframe containing unseen observations
unseen <- data.frame(Sepal.Length = c(5.3, 7.2), 
                     Sepal.Width = c(2.9, 3.9), 
                     Petal.Length = c(1.7, 5.4), 
                     Petal.Width = c(0.8, 2.3))

# Predict the label of the unseen observations
results <- as.matrix(predict(tree, unseen, type = "class"))
```

#### Results:

```{r echo = FALSE, results='asis'}
# Print out the results
print(xtable(results, caption = "Resultant labels of Unseen Observations"), comment = FALSE)
```


### Exercise 7: How to do unsupervised lernin

In this exercise, we will group cars based on their horsepower and their weight by using the cars data frame, which has been derived from the `mtcars` dataset. To cluster the different observations in 2 groups , we will once again use `kmeans()`.

#### Instructions:

* Explore the dataset using `str()` and `summary()`. 
* Use `kmeans()` with two arguments to group the cars into two clusters based on the cars' __hp__ and __wt__, and assign the result to the `km_cars` variable. 
* Print out the cluster element of the `km_cars` variable to see which cars belong to which clusters.

```{r exercise_7, echo = TRUE}
# Create a subset of the mtcars datset with just wt and hp
cars <- subset(mtcars, select = c(wt, hp))

# Set random seed. Don't remove this line.
set.seed(1)

# Explore the cars dataset
#str(mtcars)
#summary(mtcars)

# Group the dataset into two clusters: km_cars
km_cars <- kmeans(cars, 2)

```

#### Results:

```{r echo = FALSE, results = 'asis'}
# Print out the contents of each cluster
results <- as.matrix(km_cars$cluster)
print(xtable(results, caption = "Output of the the two clusters of cars"), comment = FALSE)
```

As can be senn from the example, the __Ferrari Dino__ is in cluster __2__, while the __Fiat X1-9__ is grouped in cluster __1__. However, vizualizing the results, provides a more comprehensive overview.

### Exercise 8: Visualizing Unsupervised Learning Results

In the previous exercise, we grouped the cars based on their horsepower and their weight. An important part of machine learning is understanding the results of the model. In the case of clustering, visualization is key to interpretation! One way to achieve this, is by plotting the features of the cars and coloring the points based on their corresponding cluster.

In this exercise we summarize the reuslts of `kmeans()` in a comprehensive figure.

#### Instructions:

* Use the `plot()` command by coloring the cars based on their cluster. Do this by setting the col argument to the cluster partitioning vector: `km_cars$cluster.` 
* Print out the clusters' centroids, which are kind of like the centers of each cluster. They can be found in the centers __element__ of `km_cars`. 
* Create `points()` with the clusters's centroids. This will add the centroids to the earlier plot.

__Note:__ To learn about the other parameters that have been defined for you, have a look at the [__graphical parameters documentation__](http://www.rdocumentation.org/packages/graphics/functions/par). 

* If given a vector with a length equal to the number of plotted observations, the `col` argument will color each observation based on the corresponding element in the given vector. In this case, the coloring vector `km_cars$clusters` only contains 1's and 2's, which correspond to the colors "black" and "red" respectively. Hence, the objects in cluster one will be colored black, while those in cluster two will be colored red.
* The graphical parameter `pch` sets the points' symbol, in this case a filled square. 
* The graphical parameter `bg` sets the fill color of the points. Only applicable for pch symbols 21 through 25. 
* The graphical parameter `cex` sets the size of the points' symbol. In this case, it enlarges the symbol by 100%.

#### Results:

```{r exercise1_8, echo = TRUE}
# Create a subset of the mtcars datset with just wt and hp
cars <- subset(mtcars, select = c(wt, hp))

# Set random seed. Don't remove this line
set.seed(1)

# Group the dataset into two clusters: km_cars
km_cars <- kmeans(cars, 2)

# Plot and color the points in the plot based on the clusters
plot(cars, col = km_cars$cluster)

# Print out the cluster centroids
km_cars$centers

# Add the centroids to the plot
points(km_cars$centers, pch = 22, bg = c(1, 2), cex = 2)

```

__The cluster centroids are typically good representations of all the observations in a cluster. They are often used to summarize clusters.__

\newpage

# Chapter 2: Measuring Performance (Measuring the Error)
## Introduction

How do we assess the quality of a particular Machine Learning model after ir has been "learned"? To answer this question, we have to note the following:

1. Define what is meant by Model quality or performance. The definition depends on the context on what the Model is being used. In most cases, this is described as:
- __Model Accurancy__
- __Computation Time__ 
- __Interpretability__ 
2. The different tasks of Machine Learning have their own perfmance measures.

## Measuring Performance: Classification

In Classification systems, __Accurancy__ and __Error__ are the basic measures of performance as they reflect the number of times the system is right or wrong. Basically, the __Accuracy__ goes up when the __Error__ rate goes down, therefore:

$$Accuracy = \frac {Correctly \ Classified \ Instances}{Total \ Classified \ Instances}$$

__AND__

$$Error \ Rate = 1 - Accuracy$$

However, depending ont he context of the Model it is always a good practice to calculate the __Confusion Matrix__. The Confusion Matrix consists of rows and colums, both contain all the available labels. Each cell contains the frequency of which the instances are classified in a particular way. 

\begin{figure}[h]
\center
\includegraphics{figures/confusionmatrix.png}
\caption{Example of a Confusion Matrix}
\label{Figure2}
\end{figure}

\newpage

# Appendix A: Regression Coefficients

As an example of trying to fit a linear function between a `Predictor` (e.g. Weight) and a `Response` (e.g. Height). $$Height \approx \beta_0+\beta_1\times Weight$$ Together $\beta_1$ and $\beta_0$ are known as the model coefficients or parameters. As soon as we know what these coefficients are, the function is able to convert any new input to output. This means that solving the Machine Learning problem is to find good values for $\beta_1$ and $\beta_0$, estimated from previous input to previous output observations. 

