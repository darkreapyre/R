---
title: "My Model Selection and Regularization in R Notes"
output: 
    pdf_document:
        toc: true
        latex_engine: xelatex
    fontsize: 11pt
    geometry: margin=1in
---

\pagebreak

## Overview

The data for this exercise will be from the `ISLR` package, specifically the `Hitters` data-set, which is the Major League Baseball Data for different Baseball players from the 1986 an 1987 sessions.

```{r packages, echo = TRUE}
# Load the required packages
require(ISLR)
require(leaps)
require(glmnet)

#View the data
summary(Hitters)

```

__Note__ from the summary that there is missing data, specifically in __Salary__ column. Therefore, before proceeding, we must remove the missing values as __Salary__ will be used as the response variable for the Regression Model. Although there are many ways to deal with Missing Values, for the sake of this exercise, we will take the easy way out and simply remove any row that has missing values.

```{r clean, echo = TRUE}
#View current amount of NA's in Salary
with(Hitters, sum(is.na(Salary)))

#Remove NA's in Salary
Hitters <- na.omit(Hitters)

#Confirm the NA's have been removed
with(Hitters, sum(is.na(Salary)))

```

# Best Subset Regression  
## Overview 

Best Subset Regression looks at all the possible regression models of different subsets of size and then looks for the best model of each size. It's output is a sequence of models, which is the best for each particular size. The `leaps` package allows this to be done computationally in __R__, specifically the `regsubsets()` function. 

## Finding the best model

```{r regfit, echo = TRUE}
#Execute the best subset regression with Salary as the response
regfit <- regsubsets(Salary~., data = Hitters)

#Summary
summary(regfit)

```

__Note__ that by default it gives the best-subsets up to size __8__. Since we have __19__ variables we execute the test with all variables.

```{r regfitfull, echo = TRUE}
#execute the best subset regression with Salary as the response and all 19 variables
regfit.full <- regsubsets(Salary~., data = Hitters, nvmax = 19)

#Summary
fit.summary <- summary(regfit.full)
fit.summary

#Plot CP Statistic 
plot(fit.summary$cp, xlab = "No. of Variables",
     ylab = "Estimate of Prediction Error (CP)")

```

__Note__ that the idea is to pick a model with the lowest __CP__ and as can be seen from the plot, the model with __10__ variables is the smallest. This can be verified by using the `which.min()` function.  

Index of the smallest element of the CP component: __`r which.min(fit.summary$cp)`__


```{r pointplot, echo = TRUE}
#Redo the plot
plot(fit.summary$cp, xlab = "No. of Variables",
     ylab = "Estimate of Prediction Error (CP)")
#Highlite the point 10
points(10, fit.summary$cp[10], pch = 20, col = "red")

```

__Note__ that the `regsubsets()` function has it's own, built-in plotting method. 

```{r regfitplot, echo = TRUE, fig.height = 8, fig.width = 7}
#Plot using the regsubsets() plotting method
plot(regfit.full, scale = "Cp")

```

The plot above is a pattern picture where __Y Axis__ shows the __Cp Statistic__ where small is good, so __5__ corresponds to the model of size __10__. For each value, black squares indicate that the particular variable is "in" while the white squares indicate the particular variable is "out". __Note__ that "bad" cp's correspond to all models that have all the variables "in" or hardly any variables "in".  

## Determining the coefficients

Having chosen model __10__, there is a method for `regsubsets()` to display the coefficients of the particular model.

```{r coef, echo = TRUE}
#show the coefficients for model indexed 10
coef(regfit.full, 10)

```

# Forward Stepwise Selection
## Overview

Best Subset Regression is quite aggressive in that it looks at ALL possible subsets, Forward Step-wise Selection on the other hand is a greedy algorithm in that each time it runs, it includes the next best variable BUT produces a nested sequence. Therefore it is a much "less adventurous" search, it just adds the next best variable that fits the most. 

## Finding the best Model

Here we continue to use the `regsubsets()` function, but we specify the `method = forward` option.

```{r regfitfwd, echo = TRUE}
#execute regsubsets() with forward option
regfit.fwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)

```

__Note__ how the models that are selected are exactly nested. So each new model includes all the variables that were before plus one new one.

## Plotting the "CP Statistic"

```{r fwdplot, echo = TRUE, fig.width = 7, fig.height = 8}
#plot the fit
plot(regfit.fwd, scale = "Cp")

```

__Note__ that the resulting plot looks very similar to what we saw with "Best Subsets" with the same structure around the small end.

# Model Selection using a Validation Set
## Overview

Here we pick a subset of the variables and put them aside to be used as a validation set and the rest will be used as a training data set. This way we can choose a good subset model. 

__Note: This approach is slightly different from the approach used in the Text Book.__

## Creating the Validation and Test Sets

Since there are __`r dim(Hitters)[1]`__ rows of data, we break it down into $\frac{2}{3}$ __Training__ and $\frac{1}{3}$ __Test__. So $\frac{2}{3}$ is approximately __180__ observations.

```{r validation, echo = TRUE}
#Set the seed
set.seed(1)

#Create the Training set by taking a sample of size 180
train <- sample(seq(dim(Hitters)[1]), 180, replace = FALSE)

#Create the fit on the data indexed byt the rows in "train"
regfit.fwd <- regsubsets(Salary~., data = Hitters[train, ],
                         nvmax = 19, method = "forward")
```

## Making Predictions

Now we make predictions on the observations on the __Test__ set. Since we already know that there are __19__ subset models, we set up vectors to record the errors since there is no "predict" method available for `regsubsets()`.

```{r manualpredict, echo = TRUE}
#create the vector to record errors
errors <- rep(NA, 19)

#Create the "X" matrix corresponding to the validation dataset
x.test <- model.matrix(Salary~., data = Hitters[-train, ]) #Note -train

#Make the predictions for each model
for(i in 1:length(errors)){
        #Exctract the coefficients for each model of size id = i
        c <- coef(regfit.fwd, id = i)
        #Manual predictoin
        #The coefficient vector returns the subset of variables used in model i
        #Index ther names returned to get the right elements in x.text, the supbset
        #of colums in x.test that correspond to variables in the current coefficient
        #vector.
        pred <- x.test[, names(c)]%*%c #Matrix multiply these elements multiply by coef. vector
        #Populate the errors vector with the mean squared error
        errors[i] <- mean((Hitters$Salary[-train] - pred)^2)
}

#plot the root mean squared error
plot(sqrt(errors), ylab = "Root MSE", ylim = c(300, 400), pch = 19, type = "b")

```

__Note__ that the plot above is a plot of the validation error. The plot is slightly "jumpy", which indicates noise. The minimum error seems to be around __5__. The __10__ that we chose previously is a slightly higher then the minimum.

Next we overlay the Residual Sum of Squares on the same plot.


```{r RSS, echo = TRUE}
#Recreate the plot
plot(sqrt(errors), ylab = "Root MSE", ylim = c(300, 400), pch = 19, type = "b")

#add points for the RSS value for comparison less NULL Model
points(sqrt(regfit.fwd$rss[-1]/180), col = "blue", pch = 19, type = "b") #Note [-1]
legend("topright", legend = c("Training", "Validation"), col = c("blue", "black"), pch = 19)

```

__NOTE:__ When overlaying the __Residual Sum of Squares__ on the same plot, We remove the first model. This corresponds to the __NULL__ model which was not included in the manual prediction. Additionally, the __Residual Sum of Squares__ (as it must do), decreases because Forward Step-wise Selection includes that variable that improves the "fit" the most. So by definition it must decrease to show the improved __RSS__ on the training data. 

Since `regsubsets()` doesn't include a `predict()` method and since the process is manual, we will create a `predict()` function to be used in the future. 

```{r predict, echo = TRUE}
predict.regsubsets <- function(object,newdata, id, ...){
    #Take the foirmula from initial call on the fitted model e.g. Salary~.
    form <- as.formula(object$call[[2]])
    #Create a matrix of the model
    m <- model.matrix(form, newdata)
    #Extract the coefficients
    c <- coef(object, id = id)
    #Perform the MAtrix Multiplication
    m[, names(c)]%*%c
}

```

# Model Selection by Cross-Validation 
## Overview
Cross-validation is one of the preferred methods for doing model selection. In this example, we will use 10-fold Cross-validation, where we take 10 samples, with each observation being assigned a fold number. Thus 10 folds.

## Model Fit and Predictions

```{r 10-fold, echo = TRUE}
#Set the seed
set.seed(11)

#Create the folds vector of 1 to 10, with the length equal to the number of row in Hitters
folds <- sample(rep(1:10, length = nrow(Hitters)))

#Display the random assignnment of folds to each of the observations in Hitters
folds
table(folds)

#Create the MAtrix to contain the cross-validatio errors (10 rows, 19 colums)
cv.errors <- matrix(NA, 10, 19)

#Double loop
for(k in 1:10){
    #Fit the regsubsets() model with Salary as a reponse and the training data is all the
    #observations whos fold id is not equal to k i.e. train on all the observations except
    #those in the kth fold.
    best.fit <- regsubsets(Salary~., data = Hitters[folds!=k, ], nvmax = 19, method = "forward")
    
    #Iterate through each of the subsets and use the predict() method to make predictions on 
    #the observations whose fold id is equal to k.
    
    ##########################################################################################
    # Note that the predict() function is called and not predict.regsubsets(), bcause this   #
    # was written in a way hat the generic predict() function uderstands. The first argument #
    # is best.bit, which is a regsubsets() object so the generic predict() funciton knows to #
    # find the method which is predict.regsubsets().                                         #
    ##########################################################################################
    for(i in 1:19){
    pred <- predict(best.fit, Hitters[folds == k, ], id = i)
    
    #Compute the Mean Squared Error of the predictions and assign it to the kth row of cv.errors
    cv.errors[k, i] <- mean((Hitters$Salary[folds == k] - pred)^2)
    
    }
}

```

## Process the output

To process the output, we create a matrix with the means of each column. __Remember__ that there are __10__ rows and each row was the Mean Squared Error for a particular fold, but we want to average those down each columns. Then use the square root to get the Root Mean Squared Error.

```{r rmse, echo = TRUE}
#Create the output matrix
rmse.cv <- sqrt(apply(cv.errors, 2, mean))

#Plot the result
plot(rmse.cv, pch = 19, type = "b")

```

__Note__ that the curve is quite as "jumpy" as the validation curve because this is averaged over the full training set, done fold by fold and then averaged, so it's smoother. This seems to favor models of size __11__ or __12__.

# Ridge Regresson and Lasso 
## Overview

For this section we use the `glmnet` package. This package is used for fitting __Ridge__ models, __Lasso__ models and a whole class of models in-between (__elasticnet__).  

__Note__ that `glmnet` doesn't use a formula language, so we are required to give it a matrix of predictors and a response vector which need to be created.

```{r pre-req, echo = TRUE}
#Create x as the predictor matrix
x <- model.matrix(Salary~. -1, data = Hitters)

#Create y as the response vector
y <- Hitters$Salary

```

## Fit a Ridge Regression Model

The fist model we will fit it the __Ridge Regression Model__. This is achieved by calling `glmnet` with `alpha = 0`, which is the __Ridge__, while `alpha = 1` is the __Lasso__. (See the help file). 

__Note:__ For alpha's between __0__ and __1__ you get elastic net models, in-between __Ridge__ and __Lasso__.

```{r fitridge, echo = TRUE}
#Fit the Ridge Model
fit.ridge <- glmnet(x, y, alpha = 0)

#Plot the fit
plot(fit.ridge, xvar = "lambda", label = TRUE)

```

The above is a plot as a function of log of lambda and the coefficients. __Ridge Regression__ is penalized by the __Sum of Squares__ of the coefficients, basically penalties put on the sum of square of the coefficients which is controlled by the parameter lambda. So the criteria for __Ridge Regression__ is:

${minimize}\hspace{2 pt}RSS+\lambda\sum\limits_{j=1}^P\beta_{j}^2$

So it's trying to minimize the __Residual Sum of Squares__, BUT its been modified by a penalty of the __Sum of Squares__ of the coefficients. So if lambda is big, we want the sum of squares of the coefficients to be small so that it shrinks the coefficients towards zero. As lambda gets very big, the coefficients will all be zero.

So what `glmnet` does is it develops a whole path of models on a grid of values of lambda (about 100 values of `lambda`). So from the plot (reading from right to left), when the log of lambda is __12__, all the coefficients are essentially zero. As we relax lambda, the coefficients grow away from zero (in a smooth way) and the sum of squares of the coefficients get bigger until we reach a point where lambda is effectively zero and the coefficients are unregularized.

Unlike __Best Subset__ and __Forward Step-wise__ regression, which controls the complexity of a model by restricting the number of variables, __Ridge Regression__ keeps all the variables in and shrinks the coefficients toward zero. 

## Cross-Validation of Ridge Regression

So `glmnet` gives the whole path and we need to pick a value along that path. To do this, `glmnet` has a built-in function called `cv.glmnet()` which does k-fold cross-validation.

```{r cvridge, echo = TRUE}
#10-fold cross-validation (default)
cv.ridge <- cv.glmnet(x, y, alpha = 0)

#Plot
plot(cv.ridge)

```

The above plot shows the cross-validated mean squared error, dipping down. In the beginning the means squared errors is very high and the coefficients are restricted to be too small. At some point it levels off which indicates that the full model is doing a good job. __Note__ the two vertical lines. The first one is at the minimum and the other is at one standard error of the minimum (a slightly more restricted model, that does almost as well as the minimum).

## Fit a Lasso Model

Recall that __Lasso__ was similar to __Ridge Regression__, the only difference is the penalty. Instead of penalizing the sum of squares of the coefficients, we penalize the absolute values of the coefficients.

${minimize}\hspace{2 pt}RSS+\lambda\sum\limits_{j=1}^P\left|\beta_{j}\right|$

By penalizing the absolute value of the coefficients, some of the coefficients are exactly zero.

```{r fitlasso, echo = TRUE}
#Fit the Lasso Model
fit.lasso <- glmnet(x, y)

#Plot the fit
plot(fit.lasso, xvar = "lambda", label = TRUE)

```

As we can see from the plot, once again `glmnet` fits the whole path and initially all the coefficients are zero, (reading from right to left), and then they "jump" in. The top part of the plot (as a function of lambda) shows how many non-zero variables are in the model.

## Cross-Validation of Lasso Regression

```{r cvlasso, echo = TRUE}
#10-fold cross-validation (default)
cv.lasso <- cv.glmnet(x, y)

#Plot
plot(cv.lasso)

```

The cross-validation of __Lasso__ above is telling us that the best model (the minimum cross-validation areas) are at size __15__, and within one standard error, we have a model of size __5__.

__NOTE:__ If we extract the coefficients from cross-validation object, we can see the  vector of coefficients corresponding to the best model.

```{r cvcoefs, echo = TRUE}
#Extract the coeficients
coef(cv.lasso)

```

There are __5__ non-zero coefficients. SO by looking at the top row of the Cross-Validation plot, we can see that model of size __5__ corresponds to the best model.

## Using Training and Validations sets to manually select the Lambda for the Lasso Model

For this exercise we will use the __Training__ and __Validation__ sets created earlier to select the `lambda` for the __Lasso__ model. 

```{r lassotrain, echo = TRUE}
#Fit the path of Lasso models using x and y of train
lasso.tr <- glmnet(x[train, ], y[train])

#Display the results
lasso.tr

```

The fit shows us the __Degrees of Freedom__ (which is the number of non-zero coefficients), the __Percentage of Devianvce Explained__ (which is like $R^2$ for generalized linear models) and the __Lambda__ value that corresponds to that particular fit.

Next we make predictions on the __Vlaidation__ set. 

```{r lassopred, echo = TRUE}
#Predict
pred <- predict(lasso.tr, x[-train, ])

#Square the erros
rmse <- sqrt(apply((y[-train] - pred)^2, 2, mean))

#Plot the Log(Lambda)
plot(log(lasso.tr$lambda), rmse, type = "b", xlab = "Log(Lambda)")


```


__NOTE:__ When making predictions on the __Validation Set__, we get __`r dim(pred)[1]`__ observations. We also get __`r dim(pred)[2]`__ values of __Lambda__, so there are going to be __89__ different columns in the prediction matrix. When we compute the __Sum of Squared Errors__, `y[-train]` is a vector of length __83__, BUT `pred` is a matrix of __83 x 89__. So __R__ recycles the vector __89__ times and it does it column-wise, so that it can compute this difference. So the results of this is a matrix that's __83 x 89__, even though we only gave `y` as a vector.

```{r bestlambda, echo = TRUE}
#Extract the best lambda
best.lambda <- lasso.tr$lambda[order(rmse)[1]]

```

Here we extract the best __Lambda__ by extracting the `glmnet$lambda` component and indexing it by ascending order of root mean squared error and take the top value: __`r best.lambda`__.

Now we can view the coefficients that correspond to the best value of __Lambda__.

```{r, echo = TRUE}
#View the coesfficients corresponding to the best Lambda
coef(lasso.tr, s = best.lambda)

```