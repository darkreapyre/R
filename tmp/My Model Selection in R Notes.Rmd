---
title: "My Model Selection in R Notes"
author: "Trenton Potgieter"
date: "Saturday, February 21, 2015"
output: html_document
---

## Overview

The data for this exercise will be from the __ISLR__ package, specifically the __Hitters__ dataset, which is the Major League Baseball Data for different Baseball players from the 1986 an 1987 sessions.

```{r packages, echo = TRUE}
# Load the required packages
require(ISLR)
require(leaps)

#View the data
summary(Hitters)

```

__Note__ from the summary that there is missing data, specifically in __Salary__ column. Therefore, before proceeding, we must remove the missing values as __Salary__ will be used as the response variable for the Regression Model. Although there are many ways to deal with Missing Values, for the sake of this exercise, we will take the easy way out and simply remove any row that has missing values.

```{r clean, echo = TRUE}
#View current amount of NA's in Salary
with(Hitters, sum(is.na(Salary)))

#Remove NA's in Salary
Hitters <- na.omit(Hitters)

#Confirm the NA's have been removed
with(Hitters, sum(is.na(Salary)))

```

# Best Subset Regression  
## Overview 

Best Subset Regression looks at all the possible regression models of differnt subsets of size and then looks for the best model of each size. It's output is a sequence of models, wich is the best for each particular size. The __leaps__ package allows this to be done compuationally in __R__, specifically the __regsubsets()__ function. 

## Finding the best model

```{r regfit, echo = TRUE}
#Execute the best subset regression with Salary as the response
regfit <- regsubsets(Salary~., data = Hitters)

#Summary
summary(regfit)

```

__Note__ that by default it gives the best-subsets up to size 8. Since we have 19 variables we execute the test with all variables.

```{r regfit.full, echo = TRUE}
#execute the best subset regression with Salary as the response and all 19 variables
regfit.full <- regsubsets(Salary~., data = Hitters, nvmax = 19)

#Summary
fit.summary <- summary(regfit.full)
fit.summary

#Plot CP Statistic 
plot(fit.summary$cp, xlab = "No. of Variables",
     ylab = "Estimate of Prediction Error (CP)")

```

__Note__ that the idea is to pick a model with the lowest __CP__ and as can be seen from the plot, the model with __10__ variables is the smallest. This can be verified by using the __which.min()__ function.  

Index of the smallest element of the CP component: __`r which.min(fit.summary$cp)`__


```{r point.plot, echo = TRUE}
#Redo the plot
plot(fit.summary$cp, xlab = "No. of Variables",
     ylab = "Estimate of Prediction Error (CP)")
#Highlite the point 10
points(10, fit.summary$cp[10], pch = 20, col = "red")

```

__Note__ that the __regsubsets()__ function has it's own, built-in plotting method. 

```{r regfit.plot, echo = TRUE}
#Plot using the regsubsets() plotting method
plot(regfit.full, scale = "Cp")

```

The plot above is a pattern picture where __Y Axis__ shows the __Cp Statistic__ where small is good, so __5__ corresponds to the model of size __10__. For each value, black squares indicate that the particular variable is "in" while the white squares indicate the particular variable is "out". __Note__ that "bad" cp's correspond to all models that have all the varbiables "in" or hardly any varibles "in".  

## Determining the coefficients

Having choosen model __10__, there is a method for __regsubsets()__ to display the coefficients of the particular model.

```{r coef, echo = TRUE}
#show the coefficients for model indexed 10
coef(regfit.full, 10)

```

# Forward Stepwise Selection
## Overview

Best Subset Regression is quite agressive in that it looks at ALL possible subsets, Forward Stepwise Selection on the other hand is a greedy algorithm in that each time it runs, it includes the next best variable BUT produces a nested sequence. Therefore it is a much "less adventurous" search, it just adds the next best variable tht fits the most. 

## Finding the best Model

Here we continue to use the __regsubsets()__ function, but we specify the __method = forward__ option.

```{r regfit.fwd, echo = TRUE}
#execute regsubsets() with forward option
regfit.fwd <- regsubsets(Salary~., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)

```

__Note__ how the models that are selected are exactly nested. So each new model includes all the variables that were before plus one new one.

## Plotting the "CP Statistic"

```{r fwd.plot, echo = TRUE}
#plot the fit
plot(regfit.fwd, scale = "Cp")

```

__Note__ that the resulting plot looks very similar to what we saw with "Best Subsets" with the same structure around the small end.

# Model Selection using a Validation Set
## Overview

Here we pick a subset of the variables and put them asside to be used as a validation set and the rest will be used as a training data set. This way we can choose a good subset model. 

__Note: This approach is slightly different from the approach used in the Text Book.__

## Creating the Validation and Test Sets

Since there are __`r dim(Hitters)[1]`__ rows of data, we break it down into $\frac{2}{3}$ __Training__ and $\frac{1}{3}$ __Test__. So $\frac{2}{3}$ is approcimately __180__ observations.

```{r validation, echo = TRUE}
#Set the seed
set.seed(1)

#Create the Training set by taking a sample of size 180
train <- sample(seq(dim(Hitters)[1]), 180, replace = FALSE)

#Create the fit on the data indexed byt the rows in "train"
regfit.fwd <- regsubsets(Salary~., data = Hitters[train, ],
                         nvmax = 19, method = "forward")
```

## Making Predictions

Now we make predictions on the observations on the __Test__ set. Since we already know that there are __19__ subset models, we set up vectors to record the errors since there is no "predict" method available for __regsubsets()__.

```{r predict, echo = TRUE}
#create the vector to record errors
errors <- rep(NA, 19)

#Create the "X" matrix corresponding to the validation dataset
x.test <- model.matrix(Salary~., data = Hitters[-train, ]) #Note -train

#Make the predictions for each model
for(i in 1:length(errors)){
        #Exctract the coefficients for each model of size i
        c <- coef(regfit.fwd, id = i)
        #Manual predictoin 
        pred <- x.test[, names(c)]%*%c #Matrix multiply by coef. vector
        #Populate the errors vector with the mean squared error
        errors[i] <- mean((Hitters$Salary[-train] - pred)^2)
}

#plot the root mean squared error
plot(sqrt(errors), ylab = "Root MSE", ylim = c(300, 400), pch = 19, type = "b")

```

__Note__ the plot above is a plot of the validation error. The plot is slightly "jumpy", which idicates noise. The minimum error seems to be around __5__. The __10__ that we chose previously is a slightly higher then the minimum.

Next we overlay the Residual Sum of Squares on the same plot.

```{r RSS, echo = TRUE}
#Recreate the plot
plot(sqrt(errors), ylab = "Root MSE", ylim = c(300, 400), pch = 19, type = "b")

#add points for the RSS NULL MODEL???????
points(sqrt(regfit.fwd$rss[-1]/180), col = "blue", pch = 19, type = "b")

```


__NULL MODEL???????__