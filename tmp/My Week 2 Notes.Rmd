---
title: "My Week 2 Notes"
author: "Trenton Potgieter"
date: "Tuesday, April 14, 2015"
output: 
    pdf_document:
        toc: true
        fig_caption: true
        latex_engine: xelatex
    fontsize: 11pt
    geometry: margin=1in
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
# make this an external chunk that can be included in any file
require(knitr)
require(UsingR)
require(ggplot2)
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

```

\newpage

# Statistical Linear Regression Models

## Overview

Finding a simple Regression Model using __Least Squares__ is simply a mathematical proceedure. However we need to perform statistics, to draw __inferences__ on the data. 

__We want to generalize, from our data, to a population byusing statistical models.__

This is accomplished by using the probabilitic model for linear regression:

$$Y_i = \beta_o + \beta_1 X_i + \epsilon_i$$

1. The response ($Y_i$) is the line ($\beta_0 + \beta_1 X_i$), plus the error term.
  * $\beta_0$ and $\beta_1$ are unknown. 
  * $\beta_0$ and $\beta_1$ are the population parameters wo estimate.
  * $X_i$ is a known collection collection of explainitory variables.
  * To make is a statistical model, we add __Gaussian iid__ errors ($\epsilon_i$).

2. $\epsilon_i$ is assumed to be iid Normal $N\left(0, \sigma^2 \right)$.
  * One can think of these errors as an accumulation of the variables that weren't part of the model, but still affect the response.

3. Note that the Expected value of the response given a particular value of the regressor, is simply the line.
  * $E\left[Y_i \mid X_i = x_i\right] = \beta_0 +\beta_1 x_i$
  * Remember that $\beta_0 + \beta_1 x_i = \mu_i$.
  * The least squares estimate for $\mu_i$ is esactly the maximum likelihood estimate.
  * Remember (from the Statistical Inference class) that the maximum likeligood estimates of $\beta_0$ and $\beta_1$ as the least squares estimates:
  
$$\hat \beta_1 = Cor\left(Y, X\right) \frac{Sd \left(Y\right)}{Sd \left(X\right)} ~~~ \hat \beta_0 = \hat Y - \hat \beta_1 \hat X$$

4. Note that the Variance of the response at a given value of the regressor, is $\sigma^2$.
  * $Var\left(Y_i \mid X_i = x_i\right) = \sigma^2$
  * This ios the Variance aroind the regressio line, no the Variance of the response.
  * It will belower then the Variance of the response, due to the fact that some of the variation has been explained away by conditioning on $x_i$.

5. Both the Expected Value and the Variance are population quantioties that we would like to know.

## Interpreting the Regression Coefficients

Now that we have defines a formal statistical framework, we can interpret the regression coefficients with respect to the framework.

1. $\beta_0$ is the expected value of the response when the predictor is $0$.
2. $\beta_1$ is the expected change in the response per unit change in the predictor or regression variable.
3. Shifting the regressor by a constant doesn't change the slope, however it does change the ingtercept. Shifting the regressor ($X$) values by a constant value ($\alpha$) changes the value of the intercept, but not the value of the slope.
4. Often $\alpha$ is set to the the average $\hat X$, so that the intercept is interpretted as the expected value of the response at the average $X$ value.

## Using Regression Coefficients for Prediction

1. Basically we can get a prediction for $Y_i$ by taking $\hat \beta_0 + \hat \beta_1$ and multiplying it by the $X$ that we want to predict at:

$$\hat Y\left(X\right) = \hat \beta_o + \hat \beta_1 X$$

__NOTE:__ That the $\beta$ vlues have "hats" becuase they are the estimations, spcifically the __least squares__ estimation.

2. For the fitted values or predictions, we simply plug in the observed $X$ values.

$$\hat \mu_i = \hat Y = \hat \beta_0 + \hat \beta_1 X_i$$

## Examples of Interpreting and Using the Regression Coefficients 

To show this, we use the `diamond` data set from the `UsingR` package. The data shows diamond prices (in Singapore Dollars) and diamond wiehgt in carats (the standard measure of diamon mass, 0.2$g$). 

```{r fit1, echo = TRUE}
#Load the data
attach(diamond)

#Plot the data
g = ggplot(diamond, aes(x = carat, y = price))
g = g + xlab("Mass (carats)")
g = g + ylab("Price (SING $)")
g = g + geom_point(size = 5, colour = "black", alpha = 0.2)
g = g + geom_point(size = 4, colour = "blue", alpha = 0.2)
g = g + geom_smooth(method = "lm", colour = "red")
g

#Model Fit
fit1 <- lm(price ~ carat)
coef(fit1)

```

The above plot shows the fitted line, the line that minimizes the sqaured vertical distances between the $\left(X, Y\right)$ points and the line.  

* From the regression variable (carat), we estimate an expected __`r round(coef(fit1)[2], 2)`__ (SING) dollar increase in price for every carat increase in mass of diamond.
* The intercept __`r round(coef(fit1)[1], 2)`__ is the expected price of a $0$ carat diamond. This is not particularly interesting as we are not really interested in $0$ carat diamonds.
* Therefore. the [Interpreting the Regression Coefficients][] section highlights that $\alpha$ is set to the average to __"mean center"__ the $X$ variable so that the intercept is on a more interpretable scale, shown below.

```{r fit2, echo = TRUE}
#Get a more interpretable intercept
#Include arithmetic function inside the formula with I()
fit2 <- lm(price ~ I(carat - mean(carat)))
coef(fit2)

```

Notice that the slope (__`r round(coef(fit2)[2], 2)`__) remains the same, but the intercept is now  __`r round(coef(fit2)[2], 2)`__. Thus, __`r round(coef(fit2)[2], 2)`__ (SING) dollar is the expected average price for the average sized diamond (0.2042 carats according to the data source). 

So how do we use this data? Imagine if someone came to us with three new diamonds with a value of $0.16$ carats, $0.27$ carats and $0.34$ carats. How could we use the fitted model to estimate the price? We could do this in two ways:

1. We could to this manually by pluging the data into the equation $\beta_0 + \beta_1 X$.
2. We could use the `predict()` function.

```{r predict, fig.height = 7, fig.width = 6, echo = TRUE}
#New diamonds
new.x <- c(0.16, 0.27, 0.34)

#Manual calculation
coef(fit1)[1] + coef(fit1)[2] * new.x

#predict() function with the new values of X for the carat variable
predict(fit1, newdata = data.frame(carat = new.x))

#Plot of what the prediction is actually accomplishing
#Predicted values at the observed Xs (red) and at the new Xs (lines)
plot(diamond$carat, diamond$price,  
     xlab = "Mass (carats)", 
     ylab = "Price (SIN $)", 
     bg = "blue", 
     col = "black", cex = 1.1, pch = 21,frame = FALSE)
abline(fit1, lwd = 2)
points(diamond$carat, predict(fit1), pch = 19, col = "red")
lines(c(0.16, 0.16, 0.12), 
      c(200, coef(fit1)[1] + coef(fit1)[2] * 0.16,
      coef(fit1)[1] + coef(fit1)[2] * 0.16))
lines(c(0.27, 0.27, 0.12), 
      c(200, coef(fit1)[1] + coef(fit1)[2] * 0.27,
        coef(fit1)[1] + coef(fit1)[2] * 0.27))
lines(c(0.34, 0.34, 0.12), 
      c(200, coef(fit1)[1] + coef(fit1)[2] * 0.34,
        coef(fit1)[1] + coef(fit1)[2] * 0.34))
text(new.x, rep(250, 3), labels = new.x, pos = 2)

```

\newpage

# Residuals and Residual Variation

## Overview

The previous section discussed the regression model fit using __least squares__ regression. In reality, the model doesn't always fit the data perfectly. To this end we account for the errors, $\epsilon_i$. Thus the full model is:

$$Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$$

It is important to note that we will never fully understand the exact error (thus clasified as noise), so we therefore estiamte the error. This is called the residual or $e_i$, where $e_i = Y_i - \hat Y_i$. Basically the resaidual is the __vertical__ distance between the observed data $Y_i$ and the estimated/predicted outcome $\hat Y_i$ which is on the regression line.

__NOTE:__  

$e_i$ IS NOT $\epsilon_i$. $\epsilon_i$ is the true error that we don't get to know because we don't even know if the model is $100\%$ correct. __It's an estimate of the error.__

So we can think of __least squares__ as mimimizing the sum of the squared residuals:

$$\sum_{i = 1}^n e_i^2$$

## Properties of the Residuals  

* Residuals are useful for investigating a poor model fit.
* Residuals can be thought of as the outcome $\left(Y\right)$ with the __linear association__ of the predictor $\left(X\right)$ removed. The phrase that is often is used is the "$\left(Y\right)$ adjusted for $\left(X\right)$".
* Positive residuals are above the line and negative residuals are below the line.
* Residual plots highlight poor model fit in a way a scatter plot can't.

## Examples of Residuals

### Basic Residual Plot

```{r residuals, echo = TRUE}
#Define the X and Y Variables
y <- diamond$price
x <- diamond$carat
n <- length(y)

#Fit the model
fit <- lm(y ~ x)

#Define the residuals
e <- resid(fit)

#Predict the response of the fitted X's
yhat <- predict(fit)

#To demonstrate the resduals are wexactly the differnce between the observed
#and fitted values by looking at the largest difference
max(abs(e - (y - yhat)))

#Compare the result to manually pluging in the values to the equation
#$\hat \epsilon = Y - \hat \beta_0 - \hat \beta_1 X$
max(abs(e - (y - coef(fit)[1] - coef(fit)[2] * x)))

```

__Residuals are the signed length of the red lines in the plot__

```{r, fig.height = 6, echo = TRUE}
#Plot the residuals
plot(diamond$carat, diamond$price,  
     xlab = "Mass (carats)", 
     ylab = "Price (SIN $)", 
     bg = "lightblue", 
     col = "black", cex = 2, pch = 21,frame = FALSE)
abline(fit, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(y[i], yhat[i]), col = "red" , lwd = 2)

```

### Residual Plot

```{r, fig.height = 6, echo = TRUE}
#Plot the residuals vs. X
plot(x, e,  
     xlab = "Mass (carats)", 
     ylab = "Residuals (SIN $)", 
     bg = "lightblue", 
     col = "black", cex = 2, pch = 21,frame = FALSE)
abline(h = 0, lwd = 2)
for (i in 1 : n) 
  lines(c(x[i], x[i]), c(e[i], 0), col = "red" , lwd = 2)

```

### Heteroskedasticity

A case where a model is not right because the variance isn't constant as a function of $X$.

```{r, fig.height = 6, echo = TRUE}
#Plot hetroskedasticity
x <- runif(100, 0, 6); y <- x + rnorm(100,  mean = 0, sd = .001 * x); #The 
#standard deviation of the normal actually depends on X
g <- ggplot(data.frame(x = x, y = y), aes(x = x, y = y))
g <- g + geom_smooth(method = "lm", colour = "black")
g <- g + geom_point(size = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g

```

The plot looks like a perfect regression model fit with no errors. But the following residual fit shows the hetroskedasticity.

```{r, fig.height = 6, echo = TRUE}
#Residual plot
g <- ggplot(data.frame(x = x, y = resid(lm(y ~ x))), 
           aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0, size = 2); 
g <- g + geom_point(size = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g <- g + xlab("X") + ylab("Residual")
g

```

## Estimating Residual Variation

Residuals are useful for estimating residual variation. The __Maximum Likelehood Estimate__ (MLE) for the variation ($\sigma^2$) is the __average sqaured residual__:

$$\frac{1}{n} \sum_{i=1}^n e_i^2$$

However, for regression, most people use:

$$\hat \sigma^2 = \frac{1}{n-2}\sum_{i = 1}^n e_i^2$$

This is not the average (over $n$), but $n-2$ because the MLE is biased in this case and if we want the estimate to be unbiased - its' exapected value ($E\left[\hat \sigma^2\right]$) to actually be equal to $\sigma^2$ - then we have to subtract off $2$ (one degree of freedom for the intercept and one degree of freedom for the slope). And bevcause we have to estimate those two parameter, they get subtracted in essence. 

__NOTE:__ This is exactly the same as Bessel's correction ($n-1$) to provide an unbiased estiamte for the population variance.

## Residual Variation Examples

```{r ressidualvar, echo = TRUE}
#Create the x and y variables from the diamon data
y <- diamond$price
x <- diamond$carat
n <- length(y)

#Create the fit
fit <- lm(y~x)

#View the variance (sigma)
summary(fit)$sigma

#Manaually plug into the equation to for proof to get sigma
sqrt(sum(resid(fit)^2) / (n - 2))

```

## Summarizing Variation

$$
\sum_{i=1}^n (Y_i - \bar Y)^2 
= \sum_{i=1}^n (Y_i - \hat Y_i)^2 + \sum_{i=1}^n  (\hat Y_i - \bar Y)^2 
$$

__Where:__

- The total variability in our response is the variability around an intercept
(think mean only regression).

$$\sum_{i=1}^n (Y_i - \bar Y)^2$$

- The regression variability is the variability that is explained by adding the
predictor.

$$\sum_{i=1}^n  (\hat Y_i - \bar Y)^2$$

- The error variability is what's leftover around the regression line.

$$\sum_{i=1}^n (Y_i - \hat Y_i)^2$$

__Therefore:__

Total Variation = Residual Variation + Regression Variation

So the percent of __Total Variation__ expalined by the model is:

$$R^2 = \frac{\sum_{i=1}^n \left(\hat Y_i - \bar Y\right)^2}{\sum_{i=1}^n \left(Y_i - \bar Y\right)^2} = 1 - \frac{\sum_{i=1}^n \left(Y_i - \hat Y_i\right)^2}{\sum_{i=1}^n \left(Y_i - \bar Y\right)^2}$$

R squared is the percentage of the total variability that is explained by the linear relationship with the predictor. See Appendix A for an explanation of the above equation.

## Some facts about $R^2$

- $R^2$ is the __percentage__ of variation explained by the regression model.
- $0 \leq R^2 \leq 1$ = $R^2$ has to be between zero and one.
- $R^2$ is the __sample__ correlation squared.
- $R^2$ can be a misleading summary of model fit.
    - Deleting data can inflate $R^2$.
    - Adding terms (random noise) to a regression model always increases $R^2$. (Disccused later.)
- Do `data(anscombe); example(anscombe)` below to see the following data.
    - Basically same mean and variance of $X$ and $Y$.
    - Identical correlations (hence same $R^2$ ).
    - Same linear regression relationship.

```{r, echo = FALSE, fig.height=5, fig.width=5, results='hide'}
require(stats); require(graphics); data(anscombe)
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
  #print(anova(lmi))
}

## Now, do what you should have done in the first place: PLOTS
op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13))
  abline(mods[[i]], col = "blue")
}
mtext("Anscombe's 4 Regression data sets", outer = TRUE, cex = 1.5)
par(op)
```

\newpage

# Inference in Regression  

## Review of Statistical Inference 

(See Appendix B for additonal reference and recap)

* Statistics like $\frac{\hat \theta - \theta}{\hat \sigma_{\hat \theta}}$ often have the following properties.
    1. Is normally distributed and has a finite sample Student's T distribution if the  variance is replaced with a sample estimate (under normality assumptions).
    2. Can be used to test $H_0 : \theta = \theta_0$ versus: 
        * $H_a : \theta > \theta_0$
        * $H_a : \theta < \theta_0$
        * $H_a : \theta \neq \theta_0$.
    3. Can be used to create a confidence interval for:
    
        $\theta$ via $\hat \theta \pm Q_{1-\alpha/2} \hat \sigma_{\hat \theta}$
        
        where $Q_{1-\alpha/2}$ is the relevant quantile from either a normal or T distribution.
    
* In the case of regression with iid sampling assumptions and normal errors, our inferences will follow
very similarily to what you saw in your inference class.
* We won't cover asymptotics for regression analysis, but suffice it to say that under assumptions 
on the ways in which the $X$ values are collected, the iid sampling model, and mean model, 
the normal results hold to create intervals and confidence intervals.

## Applying Statistical Inference to Regression

* $\sigma_{\hat \beta_1}^2 = Var(\hat \beta_1) = \sigma^2 / \sum_{i=1}^n (X_i - \bar X)^2$
* $\sigma_{\hat \beta_0}^2 = Var(\hat \beta_0)  = \left(\frac{1}{n} + \frac{\bar X^2}{\sum_{i=1}^n (X_i - \bar X)^2 }\right)\sigma^2$
* In practice, we don't actually know $\sigma$ so we replace it by its estimate.
* It's probably not surprising that under iid Gaussian errors:
$$
\frac{\hat \beta_j - \beta_j}{\hat \sigma_{\hat \beta_j}}
$$
follows a $t$ distribution with $n-2$ degrees of freedom and a normal distribution for large $n$.

* This can be used to create confidence intervals and perform hypothesis tests.

## Example

Usign the `diamond` data set, we can prove the above. 

```{r test1, echo = TRUE}
#Create the variables
y <- diamond$price
x <- diamond$carat
n <- length(y)

#Use lm() to get beta or manually define the equation 
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)

#Define the residuals
e <- y - beta0 - beta1 * x

#Define the estimated residual variance
sigma <- sqrt(sum(e^2) / (n-2))

#Sum of the squared x's
ssx <- sum((x - mean(x))^2)

#Standard error for Beta0
seBeta0 <- (1 / n + mean(x) ^ 2 / ssx) ^ .5 * sigma

#STandard error for Beta 1
seBeta1 <- sigma / sqrt(ssx)

#Hypothesesis testing t-distribution for beta0 and beta1.
#$H_0: \beta_0 = 0$ vs. $H_a: \beta_0 > < \neq 0$
tBeta0 <- beta0 / seBeta0
tBeta1 <- beta1 / seBeta1

#Prability value for beta0 and beta1
pBeta0 <- 2 * pt(abs(tBeta0), df = n - 2, lower.tail = FALSE)
pBeta1 <- 2 * pt(abs(tBeta1), df = n - 2, lower.tail = FALSE)

#Create a table of all the values
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0),
                   c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std. Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
coefTable

```

Verify the above proof using the `lm()` function.

```{r test2, echo = TRUE}
#Fit the model
fit <- lm(y~x)
summary(fit)$coefficients

```

\newpage

## Getting a Confidence Interval

```{r CI, echo = TRUE}
#Create a varbiable for the coefficients
sumCoef <- summary(fit)$coefficients

#Extract the relevant data for the confidence interval calculation
sumCoef[1,1]+ c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1, 2]

#Confidence interval for beta1
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2, 2]) / 10

```

So with __95%__ confidence, we estimate that a __0.1__ carat increase in diamond size results in a __`r round((sumCoef[2,1] - qt(.975, df = fit$df) * sumCoef[2, 2]) / 10, 1)`__ to __`r round((sumCoef[2,1] + qt(.975, df = fit$df) * sumCoef[2, 2]) / 10, 1)`__ increase in price in (Singapore) dollars.

\newpage

# Appendix A: $R^2$ Key

- $Y_i$ = $i^th$ observation of $Y$
- $\hat Y_i$ = Predicted value of the $i^th Y$
- $n$ = Number of observations
- $\bar Y$ = Empirical mean of $Y$
- Total Variation = $\sum_{i=1}^n \left(\bar Y - Y_i\right)^2$
- Residual Variation = $\sum_{i=1}^n \left(\hat Y_i - Y_i\right)^2$
- Regression Variation = $\sum_{i=1}^n \left(\hat Y_i - \bar Y\right)^2$

__Therefore:__

$$R^2 = \frac{Regression Variation}{Total Variation} = 1 - \frac{Residual Variation}{Total VAriation}$$

## How to derive R squared
$$
\begin{aligned}
\sum_{i=1}^n (Y_i - \bar Y)^2 
& = \sum_{i=1}^n (Y_i - \hat Y_i + \hat Y_i - \bar Y)^2 \\
& = \sum_{i=1}^n (Y_i - \hat Y_i)^2 + 
2 \sum_{i=1}^n  (Y_i - \hat Y_i)(\hat Y_i - \bar Y) + 
\sum_{i=1}^n  (\hat Y_i - \bar Y)^2 \\
\end{aligned}
$$

****

### Scratch work
$(Y_i - \hat Y_i) = \{Y_i - (\bar Y - \hat \beta_1 \bar X) - \hat \beta_1 X_i\} = (Y_i - \bar Y) - \hat \beta_1 (X_i - \bar X)$

$(\hat Y_i - \bar Y) = (\bar Y - \hat \beta_1 \bar X - \hat \beta_1 X_i - \bar Y )
= \hat \beta_1  (X_i - \bar X)$

$\sum_{i=1}^n  (Y_i - \hat Y_i)(\hat Y_i - \bar Y) 
= \sum_{i=1}^n  \{(Y_i - \bar Y) - \hat \beta_1 (X_i - \bar X))\}\{\hat \beta_1  (X_i - \bar X)\}$

$=\hat \beta_1 \sum_{i=1}^n (Y_i - \bar Y)(X_i - \bar X) -\hat\beta_1^2\sum_{i=1}^n (X_i - \bar X)^2$

$= \hat \beta_1^2 \sum_{i=1}^n (X_i - \bar X)^2-\hat\beta_1^2\sum_{i=1}^n (X_i - \bar X)^2 = 0$

## The relation between R squared and r

Recall that $(\hat Y_i - \bar Y) = \hat \beta_1  (X_i - \bar X)$
so that
$$
R^2 = \frac{\sum_{i=1}^n  (\hat Y_i - \bar Y)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}
= \hat \beta_1^2  \frac{\sum_{i=1}^n(X_i - \bar X)^2}{\sum_{i=1}^n (Y_i - \bar Y)^2}
= Cor(Y, X)^2
$$
Since, recall, 
$$
\hat \beta_1 = Cor(Y, X)\frac{Sd(Y)}{Sd(X)}
$$

__So, $R^2$ is literally $r$ squared.__

\newpage

# Appendix B: Fitted Value Summation

## The model 

$$
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
$$

* $\epsilon \sim N(0, \sigma^2)$. 
* We assume that the true model is known (correct). 
* $\hat \beta_0 = \bar Y - \hat \beta_1 \bar X$ 
* $\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)}$. 

## Standard Errors (conditioned on $X$)

$$
  \begin{aligned}
  Var \left(\hat \beta_1\right) \
  & = Var\left(\frac{\sum_{i=1}^n \left(Y_i - \bar Y\right) \left(X_i - \bar X\right)}{\sum_{i=1}^n \left(X_i - \bar X\right)^2}\right) \\
  & = \frac{Var\left(\sum_{i=1}^n Y_i \left(X_i - \bar X\right)\right)}{\left(\sum_{i=1}^n \left(X_i - \bar X\right)^2 \right)^2} \\
  & = \frac{\sum_{i=1}^n \sigma^2 \left(X_i - \bar X\right)^2}{\left(\sum_{i=1}^n \left(X_i - \bar X\right)^2 \right)^2} \\
  & = \frac{\sigma^2}{\sum_{i=1}^n \left(X_i - \bar X\right)^2} \
  \end{aligned}
$$
































