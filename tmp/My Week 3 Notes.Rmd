---
title: "My Week 3 Notes"
output: 
    pdf_document:
        toc: true
        latex_engine: xelatex
    fontsize: 11pt
    geometry: margin=1in
---
\newpage
```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
# make this an external chunk that can be included in any file
require(knitr)
require(dplyr)
require(GGally)
require(ggplot2)
require(datasets)
require(stats)
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')
```
# Multivariate Examples 
## Example 1
For this example we will use the `swiss` data set.
```{r example1a, echo = TRUE, fig.height = 6, fig.width = 7}
#Load the Swiss Fertility Dataset
data(swiss)
str(swiss)

#Plot
g <- ggpairs(swiss, lower = list(continuous = "smooth"),params = c(method = "loess"))
g
```
### Plot Interpretation 
The Plot is fairly easy to interpret, but it is important to note that the correlation between the $X$ and $Y$ axis is show in the graph (with the confidence bands), but the empirical correlation is diagonally to the right. 

### Model Interpretation 
By using fertility as thwe outcome, `lm()` is called using __ALL__ the predictors.
```{r example1b, echo = TRUE}
summary(lm(Fertility ~ ., data = swiss))$coefficients
```
To better underatand the results, we focus on __Agriculture__ and particularly the __Estimate__ (__`r summary(lm(Fertility ~ ., data = swiss))$coefficients[2]`__). This is interpreted as the following: 

* Our model estiamtes an expected __`r sqrt(round(summary(lm(Fertility ~ ., data = swiss))$coefficients[2], 2)^2)`__ decrease (because it's negative) in standardized fertility for every 1% increase in percentage (and not proportion) of males involved in agriculture in holding the remaining variables constant. 
* The next column, Std. Error (__`r round(summary(lm(Fertility ~ ., data = swiss))$coefficients[2, 2], 2)`__), talks about how precice teh variable is. In other words, how precise the statistical veriability is. 
* To perform a hypothesis test (__t-test__) for: 
    * $H_0: \beta_{Agri} = 0$ __vs.__ $H_a: \beta_{Agri} \neq 0$. (TO basically test wether or not $\beta_{Agri}$ is zero). 
    * We take the estimate and subtract off the hypothesised value ($0$) and divide by the standard error of the estimate. 
    * So the __t statistic__ is nothing other than the estimate divided by the standard error. 
    *  `R` conveniently provides this for us: __`r summary(lm(Fertility ~ ., data = swiss))$coefficients[2, 3]`__. 
* The probability of getting such a __t statistic__ is __`r summary(lm(Fertility ~ ., data = swiss))$coefficients[2, 4]`__. 

### How Model Selection changes the Estimates 
```{r example1c, echo = TRUE}
#Model with just agiculture as apredictor
summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients
```
* The Agriculture variable alone has about the same magnitude as as it did when used with other predictors. __`r summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients[2]`__ as opposed to __`r summary(lm(Fertility ~ ., data = swiss))$coefficients[2]`__, but instead of Argriculture having a $negative$ affect on Fertility, it now has a $positive$ effect. 
* Adjusting for the additional variables actually changes the actual direction of the effect of Agriculture on Fertility. $\rightarrow$ __Simpson's Paradox__. 
* In both cases, the Agriculture coefficient is strongly statistically signigicant, `r summary(lm(Fertility ~ ., data = swiss))$coefficients[2, 4]` __AND__ `r summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients[2, 4]`. 

For more information on on how an effect can reverse itself, see [Appendix A][]. 

### Dummy Varibles 
Linear Regression Models are very flexible. For instance, we can fit __factor__ variables as regressors to determine __"analysis of variance"__, which is a special case of linear models. As an example, consider the linear model:

$$Y_i = \beta_0 + X_{i1} \beta_1 + \epsilon_i$$

* Where each $X_{i1}$ is __binary__. So it is a $1$ if measurement $i$ is in a group and 0 otherwise. 
    * For example, treated versus not treated in a clinical trial. 
    * For people in the group that recived the treatment, their mean is $E[Y_i] = \beta_0 + \beta_1$. 
    * For people in the control group, whose covariate $X_i = 0$, their mean is  $E[Y_i] = \beta_0$.
    * The linear fit work out to be $\hat \beta_0 + \hat \beta_1$ is the mean for those in the "treated" group and $\hat \beta_0$ is the mean for those in the control group. 
    * $\beta_1$ is interpretted as the $increase$ or $decrease$ in the mean reponse for those that had $X$ value of $1$ (__Treated__), comparing those in the group to those in the control group (__Not Treated__). 
* This is just a nice way to fit a two-level factor variable as a linear regression variable. It provides not only the fitted value showing the means for both of the groups __BUT__ also provides the inference for comparing the two groups automatically. 
* The __t-test__ for $\beta_1$ is identical to a two group __t-test__.
* This methadology can also be extended beyond two factors (e.g. three level factor):$$Y_i = \beta_o + X_{i1} \beta_1 + X_{i2} \beta_2 + \epsilon_i$$ 
    * If $i$ is the level $1$ factor variable: $E[Y_i] = \beta_0 +\beta_1$. 
    * If $i$ is the level $2$ factor variable: $E[Y_i] = \beta_0 + \beta_2$. 
    * If $i$ is the level $3$ factor varible: $E[Y_i] = \beta_0$. 
    * $\beta_1$ compares level $1$ with level $3$. 
    * $\beta_2$ compares level $2$ with level $3$. 
    * $\beta_1 - \beta_2$ compares level $1$ to level $2$. 

For more information on working with __Dummy Variables__, see [Appendix B][]. 

## Example 2 
For this example we will once again use the `swiss` data set, but highlight fitting multiple lines with different intercepts and differnt slopes, one for each group. To demonstrate this, we will look at the `Catholic` variable. 
```{r catholic, echo = TRUE}
#Plot a histogram of the Catholic Variable
hist(swiss$Catholic)
```
Notice that the variable is very __bimodal__ because most provices are either majority Catholic or majority Protestant $\rightarrow$ bimodal = either/or. To better demonstrate this, we create binary "Catholic" varible which is $1$ if the province is majority Catholic and $0$ if it is majority Protestant. 

```{r mutate, echo = TRUE, fig.height = 5, fig.width = 7}
#Use dplyr package to specify 1 if greater then 50% catholic
swiss <- mutate(swiss, Catholic.Bin = 1 * (Catholic > 50))

#Plot the data
g <-  ggplot(swiss, aes(x = Agriculture, y = Fertility, colour = factor(Catholic.Bin)))
g <- g + geom_point(size = 6, colour = "black") + geom_point(size = 4)
g <- g + xlab("% in Agriculture") + ylab("Fertility")
g
```
The above plot shows the two factor variables. Although there seems to be some clusters it is __NOT__ obvious if there is any linear correlation. So how do we fit a model given the following? 

$Y = Fertility$  
$X_1 = Agriculture$  
$X_2 = \begin{cases}
   1, & \text{if over 50\% Catholic} \\
   0, & \text{otherwise}
  \end{cases}$  

### Model Option 1 
$$E\left[y | x_1, x_2\right] = \beta_0 + \beta_1 X_1$$

This Will just be a line and it will disregard the religion of the proviunce entirely, since there is no $X_2$. 
```{r model1, echo = TRUE, fig.height = 5, fig.width = 7}
#Fit the model that doesn't include religoin at all
fit1 <- lm(Fertility ~ Agriculture, data = swiss)

#Plot
g1 <- g
g1 <- g1 + geom_abline(intercept = coef(fit1)[1], slope = coef(fit1)[2], size = 2, color = "red")
g1

#Coefficient Summaries
summary(fit1)$coef
```

### Model Option 2 
$$E\left[y | x_1, x_2\right] = \beta_0 + \beta_1 X_1 + \beta_2 X_2$$ 

* If $X_2 = 0$ $\rightarrow$ $\beta_0 + \beta_1 X_1 + \beta_2 \cdot 0$ (__Protestant__) 
    * $\beta_0 +\beta_1 X_1$
* If $X_2 = 1$ $\rightarrow$ $\beta_0 + \beta_1 X_1 + \beta_2 \cdot 1$ (__Catholic__) 
    * $\beta_0 + \beta_1 X_1 + \beta_2$
* Fitting this model includes $X_1$ and $X_2$ - but no interaction - will fit two models that have the __same slope__ ($\beta_1 X_1$), but __different intercepts__ $\rightarrow$ $\beta_0$ for the first and $\beta_0 + \beta_2$ for the second. 
```{r model2, echo = TRUE, fig.height = 5, fig.width = 7}
#Fit the model that has two paralle lines
fit2 <- lm(Fertility ~ Agriculture + factor(Catholic.Bin), data = swiss)

#Plot
g2 <- g
g2 <- g2 + geom_abline(intercept = coef(fit2)[1], slope = coef(fit2)[2],
                       size = 2, color = "blue") #Protestant Line
g2 <- g2 + geom_abline(intercept = coef(fit2)[1] + coef(fit2)[3],
                       slope = coef(fit2)[2], size = 2,
                       color = "red") #Catholic Line
g2

#Coefficient Summaries
summary(fit2)$coef
```

### Model Option 3 
$$E\left[y | x_1, x_2\right] = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2$$ 

* If $X_2 = 0$ $\rightarrow$ $\beta_0 + \beta_1 X_1 + \beta_2 \cdot 0 + \beta_3 X_1 \cdot 0$ (__Protestant__) 
    * $\beta_0 + \beta_1 X_1$
* If $X_2 = 1$ $\rightarrow$ $\beta_0 + \beta_1 X_1 + \beta_2 \cdot 1 + \beta_3 X_1 \cdot 1$ (__Catholic__) 
    * $\beta_0 + \beta_1 X_1 + \beta_2 + \beta_3 X_1$ __OR__
    * $\left(\beta_0 + \beta_2\right) + \left(\beta_1 + \beta_3\right)X_1$
* Fitting this model and including an interaction term ($\beta_3 X_1 X_2$), then we fit two lines, but now the two lines have __different slopes__ and __different intercepts__. 
* The coefficient in front of the "Catholic" term ($\beta_2 X_2$) is going to be the change in the intercept from "Protestant" $\beta_0$ to "Catholic" $\left(\beta_0 + \beta_2\right)$. 
* The $\beta_3$ term in front of the interaction is going to be the change in the slope going from "Protestant" $\beta_1 X_1$ to "Catholic"$\left(\beta_1 + \beta_3\right)$. 
```{r model3, echo = TRUE, fig.height = 5, fig.width = 7}
#Fit the model that has different slopes and different intercepts
fit3 <- lm(Fertility ~ Agriculture * factor(Catholic.Bin), data = swiss)

#Plot
g3 <- g
g3 <- g3 + geom_abline(intercept = coef(fit3)[1], slope = coef(fit3)[2], size = 2,
                       color = "blue") #Protestant Line
g3 <- g3 + geom_abline(intercept = coef(fit3)[1] + coef(fit3)[3], 
                          slope = coef(fit3)[2] + coef(fit3)[4], size = 2,
                       color = "red") #Catholic Line
g3

#Coefficient Summaries
summary(fit3)$coef
```

\newpage

# Appendix A 
## How the Model Effect can Reverse Itself 
Regression Model Selection is a dynamic process where we have to think about which variables to include. A scientific process of putting confounders $in$ and $out$ of the Model and thinking about what their overall effect is in order to evalaute the best model.
```{r simulation, echo = TRUE}
#Assume we ahve 100 data points
n <- 100

#The second regressor (X2) is the values 1 to 100
x2 <- 1:n

#X1 is a variable that depends on x2 and "random noise"
x1 <- .01 * x2 + runif(n, -.1, .1)

#Y is negaitvley associated with x1, but positively associated with x2
y <- -x1 + x2 + rnorm(n, sd = .01) #add random normal noise
```
__Firstly__, from the plot of $X_1$ we can see that it increases and there is the additional, random noise (around the line). 
```{r firstly, echo = TRUE}
#Plot x1
plot(x1)

```
__Secondly__, We examine the output if we fit a liniar model with $X_1$ as the only predictor.
```{r secondly, echo = TRUE}
#Fit x1 by itself
summary(lm(y ~ x1))$coef
```
What we see is that we get this enormous coefficient (__`r summary(lm(y ~ x1))$coef[2]`__), which is nothing near the $-1$ that we hope it to be. 
__Thirdly__, we fit the correct model. 
```{r therefore, echo = TRUE}
summary(lm(y ~ x1 + x2))$coef
```
Now we get the correct coefficients ($\approx -1$ and $\approx 1$), because regression is taking $x_1$ and removing the effect of $x_2$. __Lastly__, we do some plots to higlight exactly how this works. 
```{r thirdly, echo = TRUE, fig.height = 5, fig.width = 7, results = 'show'}
#Put the data into a data frame for ggplot2 plotting
dat <- data.frame(y = y, x1 = x1, x2 = x2, ey = resid(lm(y ~ x2)), ex1 = resid(lm(x1 ~ x2)))
g <- ggplot(dat, aes(y = y, x = x1, colour = x2))
g <- g + geom_point(colour="grey50", size = 5) + geom_smooth(method = lm, se = FALSE, colour = "red") 
g <- g + geom_point(size = 4) 
g
```
The plot shows $X_1$ as it realtes to $Y$ and as can be seen from the line, there is a clear, $positive$ linear relationship between the two variables. However, we can see from $X_2$ (which is the color), that there is aalse a clear positive gradient. As $Y$ goes up, so does $X_2$, but as $X_1$ goes up, so does $X_2$ $\rightarrow$ __Confounding/Correlation__. 

__Lastly__, we plot the residuals. 
```{r lastly, echo = TRUE, fig.height = 5, fig.width = 7, results = 'show'}
g2 <- ggplot(dat, aes(y = ey, x = ex1, colour = x2))  
g2 <- g2 + geom_point(colour="grey50", size = 5) + geom_smooth(method = lm, se = FALSE, colour = "red") + geom_point(size = 4) 
g2
```
As can be seen from the plot, the residual $Y$ and the residual $X_1$, there is a clear $negative$ linear relationship (with slope being $\approx -1$). Additionally, the $X_2$ variable is clearly __NOT__ related to the residual $X_1$ variable. 

So the `lm()` funciton is clearly removing the $X_2$ from both $X_1$ and $Y$ to provide the correct relationship when both variables are fitted. __BUT__ this doesn't mean that throwing every variable into the regression model is the way to go. Adding extra and unnecessary varibles introduces additional consequences. 
For example, $Z$ adds no new linear information, since it's a linear combination of variables already included. `R` just drops 
terms that are linear combinations of other terms. 
```{r z, echo = TRUE}
z <- swiss$Agriculture + swiss$Education
lm(Fertility ~ . + z, data = swiss)
```
So if we see a __NA__ coefficient in `R`, we've inluded a variable that is either numerically or exactly a linear combination of the other varibales. 

\newpage

# Appendix B 
## Dummary Variable Example: Insect Sprays 
```{r InsectSprays, echo = TRUE, fig.height = 5, fig.width = 7.5}
#Load the dataset 
data(InsectSprays)

#Plot
g <- ggplot(data = InsectSprays, aes(y = count, x = spray, fill  = spray))
g <- g + geom_violin(colour = "black", size = 2) #violin plot
g <- g + xlab("Type of spray") + ylab("Insect count")
g
```

The plot shows the sprays labelled $A$ through $F$ along with the count of insects to which each spray was applied. So how can we test the differences in each of the factor levels using linear models?

To answer this question we first habe to fit the linear model with the __Spray__ as group and not broken down by factor level. 
```{r, echo= TRUE}
summary(lm(count ~ spray, data = InsectSprays))$coef
```
__NOTE:__ Spray A is missing, because the coefficients listed are in comparison with Spray A. For eaxmple, the Spray C Estimate (__`r summary(lm(count ~ spray, data = InsectSprays))$coef[3]`__) is the change in the mean between Spray C and Spray A. Additionally the Intercept Estimate (__`r summary(lm(count ~ spray, data = InsectSprays))$coef[1]`__) is the mean for Spray A. So in other words it's the difference between Spray C and Spray A. So how do we compare the various factor levels with rach other?

To answer this, we __manually__ hard code dummy variables and not allow `R` to pick the reference level.
```{r, echo= TRUE}
#Multiple the factor level by 1 to change it from boolean to numeric
summary(lm(count ~ 
   I(1 * (spray == 'B')) + I(1 * (spray == 'C')) +  
   I(1 * (spray == 'D')) + I(1 * (spray == 'E')) +
   I(1 * (spray == 'F')) + I(1 * (spray == 'A')), data = InsectSprays))$coef
```
__NOTE:__ Even though Spray A is specifically called out as a factor level, it is still not listed. This is better shown by calling the full output and not just the coefficients.
```{r, echo= TRUE}
#Multiple the factor level by 1 to change it from boolean to numeric
summary(lm(count ~ 
   I(1 * (spray == 'B')) + I(1 * (spray == 'C')) +  
   I(1 * (spray == 'D')) + I(1 * (spray == 'E')) +
   I(1 * (spray == 'F')) + I(1 * (spray == 'A')), data = InsectSprays))
```
__NOTE:__ The Spray A coefficient shows up as __NA__. This is because Spray A is redundant. We have $6$ means and an intercept. So including Spray A is therefore redundant. So what if we do want the coefficients to be the mean for each of the groups as opposed to be being factor levels referenced to a "control" level? $\rightarrow$ __Remove the Intercept__.
```{r, echo= TRUE}
#Use "-1" to remove the intercept
summary(lm(count ~ spray - 1, data = InsectSprays))$coef

#Use dplyr package and show the means for each Spray
summarise(group_by(InsectSprays, spray), mn = mean(count))
```
__NOTE:__ The output of the means above is same as the estimates for each coefficient, subtracted from the Intercept or reference level. __BUT__ we do want to include the reference level so that we can do tests $\rightarrow$ __P Values__ being showing the differences between each factor level __t statistic__ etc. So how we "play" around with factor variables in `lm()` is very important in terms of how the final model is interpreted, what the intercept means changes significantly depending on what the reference factor level is. 

This is shown in the following example where Spray C is the reference factor level.
```{r, echo = TRUE}
#Create a copy of InsecSpray with "C" as the reference factor level
spray2 <- relevel(InsectSprays$spray, "C")
summary(lm(count ~ spray2, data = InsectSprays))$coef
```
## Summary
* If we treat Spray as a factor, `R` includes an intercept and omits the alphabetically first level of the factor or reference level.
  * All t-tests are for comparisons of Sprays versus the reference level, Spray A.
  * Emprirical mean for A is the intercept.
  * Other group means are the intercept __PLUS__ their coefficient. 
* If we omit an intercept, then it includes terms for all levels of the factor. 
  * Group means are the coefficients. 
  * Tests are tests of whether the groups are different than zero. (Are the expected counts zero for that spray.)
* If we want comparisons between, Spray B and Spray C, say we could refit the model with Spray C (or Spray B) as the reference level. 

## Caveats 
* Counts are bounded from below by $0$, violates the assumption of normality of the errors. 
  * Also there are counts near zero, so both the actual assumption and the intent of the assumption are violated. 
  * It may be a better optiohn to model the data as a __Poisson Distribution)__. 
* Variance does not appear to be constant around the mean. 
  * The means/estimates are probably correct, but the inferences aren't.
  * Perhaps taking logs of the counts would help. 
  * There are $0$ counts, so maybe `log(Count + 1)`. 
* See Week 4 Notes for Poisson GLMs for fitting count data. 