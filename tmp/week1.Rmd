---
title: "Regression Models - My Week 1 Notes"
author: "Trenton Potgieter"
date: "Monday, April 06, 2015"
output: 
    pdf_document:
        toc: true
        latex_engine: xelatex
    fontsize: 11pt
    geometry: margin=1in
---

````{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE, results='hide', error=FALSE}
# make this an external chunk that can be included in any file
library(knitr)
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')
```

\newpage

# Introduction

## Background 

The concept was first used by Francis Galton, who created the terms __Regression__ and __Correlation__ in 1885. And by making use of __Rgression__, we are provided with very interpret able models. These notes will use the data that Galton originally used as can be seen from the plot below:

```{r galton, echo = TRUE}
require(UsingR)
require(dplyr)
require(manipulate)

#Load the Data
data(galton)

#Plot the Child and parent data
par(mfrow = c(1, 2))
hist(galton$child, col = "blue", breaks = 100) #100 histogram breaks
hist(galton$parent, col = "blue", breaks = 100) #100 histogram breaks

```

The plots above do not describe the joint relationship. To understand the joint relationship we need to first understand summarizing the __marginal__. The marginal is the distribution (on the histograms) of __Children__, disregarding __Parents__ and the distribution of __Parents__, disregarding __Children__, so summarizing the marginal information in each Histogram by themselves is a way of describing the "middle" of these data sets.

To do this, let's consider the __children's heights__. So let $Y_i$ be the height of a particular __child__ $i$ for $i=1,\ldots,n$, where $n=928$ (the number of __Children__).

So to define the middle we look for the value of $\mu$ that minimizes $$\sum_{i=1}^n(Y_i -\mu)^2$$

__The sum of the squared distances between the data and the "middle" value.__

This turns out to be the center of mass of the histogram, the point that minimizes the average squared distance from all the other points (Least Squares). So in this case the answer is the __sample mean__ or $\mu=\bar{X}$.

Remember that in Statistics, $\bar{X}$ is the __sample mean__ and $\mu$ is the __population mean__.

## Proof 

To prove this we will use the `manipulate()` function in `R` to see what value of $\mu$ minimizes the sum of squared deviations. (For mathematical proof, see Appendix A)

```{r manipulate, echo = TRUE}
#Create the Hist() Function
Hist <- function(mu){        
        #Create a histogram of the child data as before
        hist(galton$child, col = "blue", breaks = 100)
        #Draw the line that can be used by manipulate
        lines(c(mu, mu), c(0, 150), col = "red", lwd = 5)
        #Clculate the mean squared error
        mse <- mean((galton$child - mu)^2)
        #Create the labels
        text(63, 150, paste("mu = ", mu))
        text(63, 140, paste("MSE = ", round(mse, 2)))
}

#To call this and use manipulate in R, simply run the following:
#manipulate(Hist(mu), mu = slider(62, 74, step = 0.5))

```

So even though this can be seen visually, by using the `manipulate()` function and manually moving around the "red line", to find the exact optimal place that will balance out the Histogram.

Below is the actual Histogram of the __Empirical Mean__ of __`r mean(galton$child)`__.

```{r empirical, echo = TRUE}
#Plot the Empirical Mean
hist(galton$child, col = "blue", breaks = 100)
mean.child <- mean(galton$child)
lines(rep(mean.child, 100), seq(0, 150, length = 100), col = "red", lwd = 5)

```

## Comparing Children's Heights vs. Parebt's Heights

### Objective 

Doing this comparison is the heart of regression, basically how do we draw a line through Galton's Data as the following example plot shows:

__NOTE:__ Size of the points represents number of points at the (X, Y) combination. Additionally the regression line is centered through the average value.

```{r comparison, echo = TRUE}
#Create the X and Y Coordinates
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)

#Capture the frequency of each occurance 
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")

#Fit a basic Linear Model using lm()
fit <- lm(y~x, data = freqData)

#Plot the data and linear fit
plot(
    as.numeric(as.vector(freqData$parent)),
    as.numeric(as.vector(freqData$child)),
    pch = 21, col = "black", bg = "lightblue",
    cex = .15 * freqData$freq,
    xlab = "parent",
    ylab = "child"
    )
    abline(fit[1], fit[2], col = "red", lwd = 2)

```

__NOTE:__ To better illustrate the impact that the least squares criteria has on the graph (using the `manipulate()` function, see Appendix B). 

### Solution

As can be seen from the plot, we have fit a linear model to the data, basically by using the following:

```{r slope, echo = TRUE}
slope <- lm(I(child - mean(child))~I(parent - mean(parent)) -1, data = galton)
slope
```

This produces the value that minimizes the least squares criteria: __0.646__

The rest of this document will show __how__ to the solution automatically and also __why__ this is the solution. 

# Important Notation and Definitions

1. $X_1,X_2,\dots,X_n$ is used to describe $n$ data points. For example the data set $\{1, 2, 5\}$, is as follows $X_1=1,X_2=2,X_3=3$ and $n=3$.
2. The Greek alphabet is used to describe population variables that are unknown. For example, $\mu$ is used to describe the estimated Population Mean.
3. Non-Greek letters are used to describe something that is observable. For example, $\bar X$

## Example 1: Finidng the empirical (sample) mean of the data set

1. Define the empirical mean as: __We take the data, add it up and divide by the nunmber of observations__

$$\bar X = \frac{1}{n}\sum_{i=1}^n X_i$$

(The sample mean is usually denoted by a letter with a "bar" over it, in this case $\bar X$ is the sample mean of the collection of $X$'s. This is equal to $\frac{1}{b}$ multiplied by all the $X$'s.)


2. If we subtract the mean form every data point, then the result is a new data set with $n$ observations, but the new data set has the mean of $0$. So the notation is defined as:

$$\tilde X_i=X_i-\bar X_i$$

(Each $X_i$ subtracting off the mean ($\bar X$), the the new data set ($\tilde X_i$) have a mean $0$.)

3. This process is refereed to as __"centering"__ random variables. which can be proved empirically by generating random vectors, subtracting the mean from each element of the vector and taking the mean of the new vector, which will always be $0$.

4. Recall from the outset that the sample mean is the __least squares__ solution. So to find the value of $\mu$ that minimizes the following equation, then that $\mu$ works out to be $\bar X$.

$$\sum_{i=1}^n\left(\bar X_i - \mu\right)^2$$

## Example 2: Finding the empirical standard deviation and variance

1. Variance is usually denoted by $S^2$ and is given by the following formula:

$$
 \begin{aligned}
 S^2 & = \frac{1}{n-1} \sum_{i=1}^n \left(X_i - \bar X\right) \\
 & = \frac{1}{n-1}\left(\sum_{i=1}^n X_i^2 - n\bar X^2\right) \
 \end{aligned}
$$

(The average squared deviation of the observations around the mean.)

2. The empirical standard deviation is simply the square root of the variance.:

$$S = \sqrt S^2$$

(Standard deviations are good to work with as the variance is expressed in whatever units $X$ has but squared, whereas the standard deviation is expressed int he normal units of $X$.)

3. Similar to __"centering"__, where we subtract the mean from every observation to produce a resulting data set of mean $0$. If we divide every observation by the standard deviation ($\frac{X_i}{S}$), the resulting data set will have a standard of $1$. This is refereed to as __"scaling"__ the data.

## Example 3: Normalization

1. If we take the original data and subtract off $\bar X$ and then take the resulting __"centered"__ data and scale it by $S$, we get a new data set ($Z_i$), and this has empirical mean $0$ and empirical standard deviation $1$, as follows:

$$Z_i = \frac{X_i - \bar X}{S}$$

2. This process of __"centering"__ and then __"scaling"__ is called __"normalizing"__ the data.

3. Normalized data is centered at $0$ and have units equal to standard deviations away from the mean. For example, a value of $2$ from randomized means that that data point was two standard deviations larger then the mean.

4. As the name suggests, normalization is an attempt to make otherwise non-comparable data sets comparable.

## Example 4: Finding the empirical covariance (correlation)

1. Empirical co-variance is the most central quantity in regression.

2. Consider two vectors $\left(X_i, Y_i\right)$ that are lined-up. So $X_1$ might be the __BMI__ for subject $1$ and $Y_1$ might be the __BLood Pressure__ for subject $1$. $X_2$ might be the __BMI__ for subject $2$ and $Y_2$ might be the __BLood Pressure__ for subject $2$ etc. 

3. This data will allow us to create a meaning scatter-plot.

4. Based on this, we can define the co-variance as follows:

$$
 \begin{aligned}
 Cov\left(X, Y\right) & = \frac{1}{n-1}\sum_{i=1}^n\left(X_i - \bar X\right) \
 \left(Y_i - \bar Y\right) \\
 & = \frac{1}{n-1} \left(\sum_{i=1}^n X_i Y_i - n \bar X \bar Y\right) \
 \end{aligned}
$$

__(Basically the Summation of the $X$ deviations from their mean multiplied by the $Y$ deviations from their mean, over $n$)__

5. The correlation is simply the co-variance standardized into a unit-less quantity:

$$Cor\left(X, Y\right) = \frac{Cov\left(X, Y\right)}{S_x S_y}$$

__(Basically, the correlation is the co-variance of $X$ and $Y$, which has units of $X$ times units of $Y$. Then we divide by the standard deviation of $x$ and the standard deviation of $y$. So we get a unit free quantity.)__

6. In other words, $S_x$ and $S_y$ are the estimated of the standard deviations from the $X$ observations and the $Y$ observations respectively.

## Basic facts about correlation

1. The correlation of $X$ and $Y$ is equal to the correlation of $Y$ and $X$:

$$Cor\left(X, Y\right) = Cor\left(Y, X\right)$$

2. Correlation of $X$ and $Y$ has to be between $-1$ and $+1$:

$$-1 \leq Cor\left(X, Y\right) \leq 1$$

3. Correlation will only achieve these bounds of $Cor\left(X, Y\right) = 1$ and $Cor\left(X, Y\right) = -1$ when the $X$ and $Y$ observations fall perfectly on a positively or negatively sloped line i.e. a positive line for a correlation of $1$ and negative line for a correlation of $-1$.

4. Correlation measures the strength of the linear relationship between $X$ and $Y$ and is __central__ to __Linear Regression__. We estimate a stronger relationship, the closer the correlation is to the extremes of $-1$ or $+1$.

5. A correlation of $0$ implies no linear relationship:

$$Cor\left(X, Y\right) = 0$$

# Least squares estimation of regression lines 

Now that there is an understanding of some of the basic definitions and notation used, consider again the parent and child height data from the Galton data.

```{r recap, echo=TRUE}
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")  
g
```

Recall from the above scatter-plot that the size of the circle represents the frequency of the particular $\left(X, Y\right)$ combination. Based on this we want to use the Parent's Height to _explain__ the Child's Height using Linear Regression.

## Fitting the best line

To begin, let $Y_i$ be the $i^{th}$ child's height and let $X_i$ be the $i^{th}$ parent's heights. Note that this is the average over both of the parent's heights. So to find the line that fits' the __best__ line using the least squares. But we want the line to look like child's height times the __intercept__ ($\beta_0$) plus the parent's height times the __slope__ ($\beta_1$).

So $\beta_0$ and $\beta_1$ are parameters that we would like to know but currently don't know and in order to find the  __"best"__ line that fits the data, we need a criteria, the least squares criteria:

$$\sum_{i=1}^n \{Y_i - \left(\beta_0 + \beta_1 X_i\right)\}^2$$

The basic idea of the equation is that we want to minimize the sum of the squared vertical distances between the height of the child ($Y_i$) and the points on the fitted line ($\beta_1 X_i$). __NOTE__ that the equation is squared because points above the line are positive AND points below the line are negative. So squaring them will always result in a positive number. For example, $-2^2 = \left(-2\right) * \left(-2\right)$ and a negative times a negative is always a positive. 

## Revisiting Galton's data

So what is the answer when performing this model fit to Galton's data?

### Double check the calculations using R

The following `R` code confirms the above mentioned equations with the built-in regression function, `lm()`.

```{r calculations, echo = TRUE}
#Simply the variables by naming the x and y variables as per the equation
y <- galton$child
x <- galton$parent

#Create the beta variables for the equation
beta1 <- cor(y, x) *  sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)

#View these parameters compared to `lm()`
z <- rbind(c(beta0, beta1), coef(lm(y ~ x)))
row.names(z) <- c("Equation", "lm()")
```

When we manually calculate the equation and extract the the coefficients from the `lm()` function, we get the same results.

```{r, echo = FALSE}
#Print results
z
```

__So the foumula holds. (See Appendix C for further notes)__

### Adding the regression line

Now that we have confirmed that the calculations are correct, we can apply the fit to the scatter-plot by adding `g <- g + geom_smooth(method="lm", formula=y~x)` to the `ggplot` object. The final plot is as follows:

```{r final, echo = FALSE}
#add the linear model to `g`
g <- g + geom_smooth(method="lm", formula=y~x, colour = "red")
g

```

__NOTE:__ `ggplot()` automatically provides a confidence interval for the regression line to show statistical uncertainty. 

# Regression toward the mean 

## Background

According to Wikipedia[^wikipedia], in statistics, regression toward (or to) the mean is the phenomenon that if a variable is extreme on its first measurement, it will tend to be closer to the average on its second measurement-and, paradoxically, if it is extreme on its second measurement, it will tend to have been closer to the average on its first.To avoid making incorrect inferences, regression toward the mean must be considered when designing scientific experiments and interpreting data. Basically, Regression to the mean (RTM) phenomenon can make natural variation in repeated data look like real change. It happens when unusually large or small measurements tend to be followed by measurements that are closer to the mean.

It is important to note that RTM[^RTM] is a ubiquitous phenomenon in repeated data and should always be considered as a possible cause of an observed change. Its effect can be alleviated through better study design and use of suitable statistical methods.

Another way to think of this is the extreme case where it's $100\%$ RTM. To do this we simulated pair of random normals $\left(x, y\right)$:

```{r rtm, echo = TRUE}
#Simulate a pair of 100 standard normals that are completely independent
x <- rnorm(100)
y <- rnorm(100)

#Take the largest x
ord <- order(x)
largest_x <- x[ord[100]] #Maximum/last x
largest_y <- y[ord[100]] #The y paired with the largest x

```

The largest first ones would be the largest by chance (e.g. x =`r largest_x`), and the probability that there are smaller for the second simulation is high (e.g. y = `r largest_y`).

 * In other words the probability $P(Y < x | X = x)$ gets bigger as $x$ heads into the very large values.
 * Similarly the probability $P(Y > x | X = x)$ gets bigger as $x$ heads to very small values.

This is the "extreme" version of RTM where there is $100\%$ RTM, however in most cases, there is a blend of some of the intrinsic components and the noise. Think of the regression line as the intrinsic part. Unless $Cor(Y, X) = 1$, so the intrinsic part isn't perfect. 

## Application of RTM to Galton's data

How did Francis Galton apply the idea regression and correlation to quantify RTM? To answer this question we look at the following example:

* Suppose that we normalize $X$ (child's height) and $Y$ (parent's height).
* The data set will a be a single parent data set where the parent's height will be that of the father.
* The data has been normalized so that they have a mean $0$ and variance $1$.
* Then, recall that because the data has been normalized, the regression line passes through $(0, 0)$ (the mean of the X and Y).
* Therefore, regardless of which variable is the outcome (recall, both standard deviations are 1), the slope of the regression line is the correlation $Cor(Y,X)$.

__NOTE:__ If $X$ is the outcome and the plot is created where $X$ is the horizontal axis, then the slope of the least squares line that is plotted is $1/Cor(Y, X)$.

```{r application, echo = TRUE}
#Load the data set. Sopecifically the father and son data set
data(father.son)

#Take son'e height as y and normaize
y <- (father.son$sheight - mean(father.son$sheight)) / 
        sd(father.son$sheight)

#Take father's height as x and normalize
x <- (father.son$fheight - mean(father.son$fheight)) /
        sd(father.son$fheight)

#(x, y) now have mean 0 and variance 1
#Create rho to represent correlations
rho <- cor(x, y)

#Plot the data using ggplot
g <- ggplot(data.frame(x = x, y = y), aes(x = x, y = y))
g <- g + geom_point(size = 6, colour = "black", alpha = 0.2)
g <- g + geom_point(size = 4, colour = "red", alpha = 0.2)
#Create the axis to be between -4 and +4 for both axis
g <- g + xlim(-4, 4) + ylim(-4, 4) #Good for a standardized variable
#Add the identity line
g <- g + geom_abline(position = "identity", colour = "purple")
#OR
#g <- g + geom_abline(intercept = 0, slope = 1)
#Add the virtical axis line at 0
g <- g + geom_vline(xintercept = 0)
#add the horizontal axis line at 0
g <- g + geom_hline(yintercept = 0)
#Blue line for son's height at the outcome and father as the predeictor
g <- g + geom_abline(intercept = 0, slope = rho, size = 2, colour = "blue")
#Green line for father's height as the outcome and son's height as the predictor
g <- g + geom_abline(intercept = 0, slope = 1 / rho, size = 2, colour = "green")
g = g + xlab("Father's height, normalized")
g = g + ylab("Son's height, normalized")
g

```

How does RTM relate to the above plot?

* If you had to predict a son's normalized height, it would be
  $Cor(Y, X) * X_i$. 
* If you had to predict a father's normalized height, it would be
  $Cor(Y, X) * Y_i$. 
* Multiplication by this correlation shrinks toward 0. (__regression toward the mean__).
* Although RTM is a fundamental concept that lead to modern Regression as a whole, and it still has it's place when studying longitudinal data. (__think of the above plot as a topogrpahical map__)


__SIDE NOTES__

If the observations fell perfectly on a line, it would have to be, in this case the identity line (Purple Line).

* This is because both the $x$ and the $y$ have been normalized.
* So if the Father's height were $2$ (and there wasn't any noise), using the identity line, we can predict that the son's height would be $2$. __(Basically moving your finger up from 2 on the X-Axis until it hits the identity line)__
* But since there is noise, the prediction falls at the blue line, where $y = x * Cor\left(x, y\right)$, which is __`r round(2 * rho)`__ if $x = 2$ in this case. __(Basically multiply the father's height of $2$ times the slope, which is the correlation, to get the child's height prediction from the blue line intercept, shich is $1$)__

\newpage

# Appendix A: Mathematical Proof for Least Squares

$$
 \begin{aligned}
 \sum_{i=1}^n \left(Y_i - \mu\right)^2 & = \
 \sum_{i=1}^n \left(Y_i - \bar Y + \bar Y - \mu\right)^2 \\
 & = \sum_{i=1}^n \left(Y_i - \bar Y \right)^2 + \
 2 \sum_{i=1}^n \left(Y_i - \bar Y \right) \left(\bar Y - \mu\right) + \
 \sum_{i=1}^n \left(\bar Y - \mu\right)^2 \\
 & = \sum_{i=1}^n \left(Y_i - \bar Y \right)^2 + \
 2 \left(\bar Y - \mu\right) \sum_{i=1}^n \left(Y_i - \bar Y \right) + \
 \sum_{i=1}^n \left(\bar Y - \mu\right)^2 \\
 & = \sum_{i=1}^n \left(Y_i - \bar Y \right)^2 + \
 2 \left(\bar Y - \mu\right) \left(\left(\sum_{i=1}^n Y_i\right) - \
 n \bar Y\right) + \
 \sum_{i=1}^n \left(\bar Y - \mu\right)^2 \\
 & = \sum_{i=1}^n \left(Y_i - \bar Y \right)^2 + \
 \sum_{i=1}^n \left(\bar Y -\mu\right)^2\\
 & \geq \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 \
 \end{aligned}
$$

\newpage

#Appendix B: Using `manipulate()` to show Least Squares

```{r B, echo = TRUE}
require(manipulate)
myPlot <- function(beta){
    y <- galton$child - mean(galton$child)
    x <- galton$parent - mean(galton$parent)
    freqData <- as.data.frame(table(x, y))
    names(freqData) <- c("child", "parent", "freq")
    plot(
        as.numeric(as.vector(freqData$parent)),
        as.numeric(as.vector(freqData$child)),
        pch = 21, col = "black", bg = "lightblue",
        cex = .15 * freqData$freq,
        xlab = "Parent",
        ylab = "Child"
        )
    abline(0, beta, lwd = 3)
    points(0, 0, cex = 2, pch = 19)
    mse <- mean((y - beta * x)^2)
    title(paste("beta = ", beta, "mse = ", round(mse, 3)))
}

#To execute, load the following using manipulate()
#manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))

```

\newpage

# Appendix C: Additional Notes

The following is a list of things to keep in mind regarding the Least Squares calculation.To reiterate, we want the line ($Y = \beta_0 + \beta_1 X$) that is fit through a scatter-plot of point $\left(X_i, Y_i\right)$ where $Y_i$ is the outcome. We put little hats on $\beta_0$ and $\beta_1$ to indicate the estimated values, $Y = \hat \beta_0 + \hat \beta_1 X$. The solutions works out to be:

$$\hat \beta_1 = Cor(Y, X) \frac{Sd(Y)}{Sd(X)} ~~~ \hat \beta_0 = \bar Y - \hat \beta_1 \bar X$$

($\hat \beta_1$ is the correlation between $Y$ and $X$ times the standard deviation of $Y$, divided by the standard deviation of $X$. The estimated intercept $\hat \beta_0$ is $\bar Y - \hat \beta_1$ times $\bar X$)

So what are the consequences of this being the result?

1. $\hat \beta_1$ has the units of $Y$ divided by the units of $X$. $\hat \beta_0$ has the units of $Y$. This is because the correlation is a unit-less quantity, the standard deviation of $Y$ has the units of $Y$ and the standard deviation of $X$ has the units of $X$. So because the slop of a line is the change in $Y$ divided by the change in $X$, $\hat \beta_1$ has to have the units of $Y / X$ units. 

2. The line always passes though the point $\left(\bar X, \bar Y\right)$.

3. If we reverse the role of $Y$ and $X$ and treat $X$ as the outcome and $Y$ as the predictor, then we simply get the answer that the slope of this particular line is $Cor\left(Y, X\right)$ times the standard deviation of $X$ divided by the standard deviation of $Y$. This is a totally different answer when $X$ is the outcome.

```{r reverse, echo = FALSE}
beta1 <- cor(y, x) *  sd(x) / sd(y)
beta0 <- mean(x) - beta1 * mean(y)
z <- rbind(c(beta0, beta1), coef(lm(x ~ y)))
rownames(z) <- c("Equation", "lm()")
z
```

4. If center the data first, the slope would still be the same. In other words, take each $X_i$ and subtract off it's average or mean, as well as take each $Y_i$ and subtract of it's average or mean, so that now the origin is exactly the mean of the data. $(X_i - \bar X, Y_i - \bar Y)$. Performing regression and forcing the line through the origin, we get the same answer.

```{r center, echo = TRUE}
yc <- y - mean(y) #Subtract off the mean of each y
xc <- x - mean(x) #Subtract off the mean of each x

#regression through the origin for the slope equations
beta1 <- sum(yc * xc) / sum(xc ^ 2)
c(beta1, coef(lm(y ~ x))[2]) #get the second coefficient from lm, the slope

#Alternative is to use lm() but subtract out the intercept
lm(yc ~ xc -1)
```

5. If we normalize the data - both center and scale it - the slope is exactly the correlation. $$\{ \frac{X_i - \bar X}{Sd(X)}, \frac{Y_i - \bar Y}{Sd(Y)}\} = Cor(Y, X)$$

```{r normalize, echo = TRUE}
#Normalize the Child and parent data so that they are in standard deviations,
#not incles 
yn <- (y - mean(y))/sd(y)
xn <- (x - mean(x))/sd(x)

#Show that the corealtin of the origional (y,x); the corealtion of the 
#normalized data and the lm() coefficient 
c(cor(y, x), cor(yn, xn), coef(lm(yn ~ xn))[2])
```

[^wikipedia]: http://en.wikipedia.org/wiki/Regression_toward_the_mean
[^RTM]: http://ije.oxfordjournals.org/content/34/1/215.full.pdf+html